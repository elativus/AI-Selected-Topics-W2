#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Publish trained model + generated datasets to Hugging Face Hub.

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç —Ä–∞—Å—Å—á–∏—Ç–∞–Ω –Ω–∞ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—è–≤–ª—è—é—Ç—Å—è –ø–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
week2_grpo_lis_train_v7_my.ipynb:

- merged –º–æ–¥–µ–ª—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: models/qwen2p5_1p5b_grpo_lis_merged)
  –ª–∏–±–æ –ø—É—Ç—å –∏–∑ results/trained_model.json
- —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏: data/test_*.jsonl
- (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) dev: data/dev_*.jsonl
- (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –ø—Ä–µ–¥—Å—ç–º–ø–ª–µ–Ω–Ω—ã–π train: data/train_*.jsonl

–ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞:

  # 1) –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è (–æ–¥–∏–Ω —Ä–∞–∑)
  huggingface-cli login
  # –∏–ª–∏:
  export HF_TOKEN=hf_xxx

  # 2) –ü—É–±–ª–∏–∫–∞—Ü–∏—è
  python publish_to_hf.py \
    --model_repo <username>/<repo-model> \
    --dataset_repo <username>/<repo-dataset> \
    --private

–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
  pip install -U huggingface_hub datasets

–ü—Ä–∏–º–µ—á–∞–Ω–∏—è:
- –î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–æ–∫ –º–æ–∂–Ω–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å:
    pip install -U "huggingface_hub[hf_transfer]"
  –∏ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å:
    export HF_HUB_ENABLE_HF_TRANSFER=1
"""

from __future__ import annotations

import argparse
import json
import os
import re
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple


# –î–û–õ–ñ–ï–ù —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å train/eval –Ω–æ—É—Ç–±—É–∫–∞–º–∏
SYSTEM_PROMPT = (
    "–û—Ç–≤–µ—á–∞–π –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ñ–æ—Ä–º–∞—Ç–µ:\n"
    "<think>\n"
    "...\n"
    "</think>\n"
    "<answer>\n"
    "...\n"
    "</answer>"
)


@dataclass
class DatasetFileInfo:
    kind: str          # "test" | "dev" | "train"
    name: str          # "easy" | "medium" | "hard" | "single" | ...
    difficulty: Optional[int]
    n: Optional[int]
    seed: Optional[int]
    path: Path


def _read_json(path: Path) -> dict:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def _maybe_extract_spec_from_filename(filename: str) -> Tuple[Optional[str], Optional[int], Optional[int], Optional[int]]:
    """
    –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∏–º—è, difficulty, n, seed –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –≤–∏–¥–∞:
      test_easy_d2_n200_seed1001.jsonl
      dev_medium_d5_n128_seed4002.jsonl
      train_single_d1-10_n2000_seed1234.jsonl  (difficulty –∑–¥–µ—Å—å –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–µ–Ω)
    """
    # test/dev pattern
    m = re.match(r"^(test|dev)_(?P<name>[a-zA-Z0-9\-]+)_d(?P<d>\d+)_n(?P<n>\d+)_seed(?P<seed>\d+)\.jsonl$", filename)
    if m:
        return m.group("name"), int(m.group("d")), int(m.group("n")), int(m.group("seed"))

    # train pattern (—á–∞—Å—Ç–æ dmin-dmax), difficulty –æ—Å—Ç–∞–≤–∏–º None
    m = re.match(r"^train_(?P<name>[a-zA-Z0-9\-]+)_d(?P<dmin>\d+)-(?P<dmax>\d+)_n(?P<n>\d+)_seed(?P<seed>\d+).*\.jsonl$", filename)
    if m:
        return m.group("name"), None, int(m.group("n")), int(m.group("seed"))

    return None, None, None, None


def discover_model_dir(
    model_dir: Optional[str],
    results_dir: str,
    fallback_models_dir: str = "models/qwen2p5_1p5b_grpo_lis_merged",
) -> Path:
    """
    1) –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω --model_dir -> –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
    2) –ò–Ω–∞—á–µ –ø—Ä–æ–±—É–µ–º results/trained_model.json (—Å–æ–∑–¥–∞—ë—Ç—Å—è train-–Ω–æ—É—Ç–±—É–∫–æ–º)
    3) –ò–Ω–∞—á–µ fallback: models/qwen2p5_1p5b_grpo_lis_merged
    """
    if model_dir:
        p = Path(model_dir).expanduser().resolve()
        if not p.exists():
            raise FileNotFoundError(f"--model_dir –Ω–µ –Ω–∞–π–¥–µ–Ω: {p}")
        return p

    trained_model_json = Path(results_dir) / "trained_model.json"
    if trained_model_json.exists():
        info = _read_json(trained_model_json)
        td = info.get("trained_model_dir")
        if td:
            p = Path(td).expanduser().resolve()
            if p.exists():
                return p

    p = Path(fallback_models_dir).expanduser().resolve()
    if p.exists():
        return p

    raise FileNotFoundError(
        "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –º–æ–¥–µ–ª–∏.\n"
        f"–ü—Ä–æ–±–æ–≤–∞–ª:\n"
        f"  - --model_dir (–Ω–µ –∑–∞–¥–∞–Ω)\n"
        f"  - {trained_model_json} (–∏–ª–∏ –ø—É—Ç—å –≤–Ω—É—Ç—Ä–∏ –Ω–µ–≥–æ)\n"
        f"  - {p}\n"
        "–£–∫–∞–∂–∏—Ç–µ --model_dir —è–≤–Ω–æ."
    )


def discover_dataset_files(data_dir: str, include_train: bool) -> List[DatasetFileInfo]:
    """
    –ò—â–µ–º jsonl, –∫–æ—Ç–æ—Ä—ã–µ –≥–µ–Ω–µ—Ä–∏—Ç train-–Ω–æ—É—Ç–±—É–∫:
      data/test_*.jsonl, data/dev_*.jsonl, (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) data/train_*.jsonl
    """
    d = Path(data_dir).expanduser().resolve()
    if not d.exists():
        raise FileNotFoundError(f"data_dir –Ω–µ –Ω–∞–π–¥–µ–Ω: {d}")

    out: List[DatasetFileInfo] = []

    for kind in ("test", "dev"):
        for p in sorted(d.glob(f"{kind}_*.jsonl")):
            name, diff, n, seed = _maybe_extract_spec_from_filename(p.name)
            out.append(DatasetFileInfo(kind=kind, name=name or p.stem, difficulty=diff, n=n, seed=seed, path=p))

    if include_train:
        for p in sorted(d.glob("train_*.jsonl")):
            name, diff, n, seed = _maybe_extract_spec_from_filename(p.name)
            out.append(DatasetFileInfo(kind="train", name=name or p.stem, difficulty=diff, n=n, seed=seed, path=p))

    # sanity: –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å test —Ñ–∞–π–ª—ã
    if not any(x.kind == "test" for x in out):
        raise FileNotFoundError(
            f"–í {d} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ test_*.jsonl.\n"
            "–°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ train-–Ω–æ—É—Ç–±—É–∫ (—á–∞—Å—Ç—å —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ç–µ—Å—Ç—Å–µ—Ç–∞–º–∏) "
            "–∏–ª–∏ —É–∫–∞–∂–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π --data_dir."
        )

    return out


def _make_model_card(model_repo: str, base_model: str, dataset_repo: str) -> str:
    return f"""---
language: ru
tags:
- reinforcement-learning
- grpo
- qwen2.5
- lis
license: other
base_model: {base_model}
---

# GRPO LIS agent (Week 2)

–≠—Ç–æ –º–æ–¥–µ–ª—å, –¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è —Å –ø–æ–º–æ—â—å—é GRPO (RL) –Ω–∞ —Å—Ä–µ–¥–µ **Longest Increasing Subsequence (LIS)**:
–ø–æ –∑–∞–¥–∞–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª –Ω—É–∂–Ω–æ –≤–µ—Ä–Ω—É—Ç—å –¥–ª–∏–Ω—É LIS.

## –í–∞–∂–Ω–æ –ø—Ä–æ —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞

–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–¥–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å train/eval):

```text
{SYSTEM_PROMPT}
```

## –î–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏

–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ test/dev –Ω–∞–±–æ—Ä—ã, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤ train-–Ω–æ—É—Ç–±—É–∫–µ, –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω—ã —Ç—É—Ç:
- `{dataset_repo}`

## –ë—ã—Å—Ç—Ä—ã–π –ø—Ä–∏–º–µ—Ä –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (Transformers)

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

repo = "{model_repo}"
tok = AutoTokenizer.from_pretrained(repo)
model = AutoModelForCausalLM.from_pretrained(repo, torch_dtype=torch.float16, device_map="auto")

user_prompt = "..."  # –≤–æ–ø—Ä–æ—Å –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–æ–¥–Ω–∞ –∑–∞–¥–∞—á–∞)
messages = [
    {{"role": "system", "content": {SYSTEM_PROMPT!r}}},
    {{"role": "user", "content": user_prompt}},
]
prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

inputs = tok(prompt, return_tensors="pt").to(model.device)
out = model.generate(**inputs, max_new_tokens=64, do_sample=False)
print(tok.decode(out[0], skip_special_tokens=True))
```
"""


def _make_dataset_card(dataset_repo: str, file_infos: List[DatasetFileInfo]) -> str:
    lines = [
        "---",
        "language: ru",
        "tags:",
        "- reinforcement-learning",
        "- grpo",
        "- lis",
        "license: other",
        "---",
        "",
        "# LIS fixed datasets (Week 2)",
        "",
        "–í —ç—Ç–æ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –ª–µ–∂–∞—Ç **—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ** –¥–∞—Ç–∞—Å–µ—Ç—ã (jsonl), –∫–æ—Ç–æ—Ä—ã–µ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –≤ train-–Ω–æ—É—Ç–±—É–∫–µ.",
        "–û–Ω–∏ –Ω—É–∂–Ω—ã –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è baseline vs trained.",
        "",
        "## –§–æ—Ä–º–∞—Ç –ø—Ä–∏–º–µ—Ä–∞",
        "",
        "–ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ JSONL –∏–º–µ–µ—Ç –≤–∏–¥:",
        "",
        "```json",
        '{"prompt": "...", "answer": "..."}',
        "```",
        "",
        "–≥–¥–µ `prompt` ‚Äî —ç—Ç–æ **user prompt** (—É—Å–ª–æ–≤–∏–µ –∑–∞–¥–∞—á–∏), –∞ `answer` ‚Äî —ç—Ç–∞–ª–æ–Ω–Ω–∞—è –¥–ª–∏–Ω–∞ LIS.",
        "",
        "## System prompt –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞",
        "",
        "–ü—Ä–∏ –æ—Ü–µ–Ω–∫–µ (—Å–º. eval-–Ω–æ—É—Ç–±—É–∫) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç:",
        "",
        "```text",
        SYSTEM_PROMPT,
        "```",
        "",
        "## –§–∞–π–ª—ã",
        "",
    ]

    # —Å–≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ kind
    by_kind: Dict[str, List[DatasetFileInfo]] = {}
    for fi in file_infos:
        by_kind.setdefault(fi.kind, []).append(fi)

    for kind in ("test", "dev", "train"):
        if kind not in by_kind:
            continue
        lines.append(f"### {kind}")
        for fi in by_kind[kind]:
            meta = []
            if fi.difficulty is not None:
                meta.append(f"difficulty={fi.difficulty}")
            if fi.n is not None:
                meta.append(f"n={fi.n}")
            if fi.seed is not None:
                meta.append(f"seed={fi.seed}")
            meta_str = (", ".join(meta)) if meta else ""
            lines.append(f"- `{kind}/{fi.path.name}` ({meta_str})")
        lines.append("")

    lines.append("## –ö–∞–∫ –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ—Ä–µ–∑ ü§ó Datasets")
    lines.append("")
    lines.append("–ü—Ä–∏–º–µ—Ä (–ø–æ–¥—Å—Ç–∞–≤—å—Ç–µ —Å–≤–æ–∏ –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤):")
    lines.append("")
    lines.append("```python")
    lines.append("from datasets import load_dataset")
    lines.append(f'repo = "{dataset_repo}"')
    lines.append('data_files = {')
    lines.append('  "test_easy": "test/test_easy_d2_n200_seed1001.jsonl",')
    lines.append('  "test_medium": "test/test_medium_d5_n200_seed2001.jsonl",')
    lines.append('  "test_hard": "test/test_hard_d8_n200_seed3001.jsonl",')
    lines.append('}')
    lines.append('ds = load_dataset(repo, data_files=data_files, split="test_easy")')
    lines.append("print(ds[0])")
    lines.append("```")
    lines.append("")

    return "\n".join(lines)


def _make_dataset_index(file_infos: List[DatasetFileInfo]) -> dict:
    idx: Dict[str, dict] = {
        "schema": {"prompt": "string", "answer": "string"},
        "system_prompt": SYSTEM_PROMPT,
        "files": {"test": {}, "dev": {}, "train": {}},
    }
    for fi in file_infos:
        entry = {
            "path_in_repo": f"{fi.kind}/{fi.path.name}",
            "filename": fi.path.name,
            "difficulty": fi.difficulty,
            "n": fi.n,
            "seed": fi.seed,
        }
        if fi.kind not in idx["files"]:
            idx["files"][fi.kind] = {}
        idx["files"][fi.kind][fi.name] = entry
    return idx


def _require_hf_libs():
    try:
        import huggingface_hub  # noqa: F401
    except Exception as e:
        raise RuntimeError(
            "–ù–µ –Ω–∞–π–¥–µ–Ω –ø–∞–∫–µ—Ç huggingface_hub. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ:\n"
            "  pip install -U huggingface_hub datasets\n"
        ) from e


def _get_token(cli_token: Optional[str]) -> Optional[str]:
    if cli_token:
        return cli_token
    for env_name in ("HF_TOKEN", "HUGGINGFACEHUB_API_TOKEN", "HUGGINGFACE_TOKEN"):
        v = os.environ.get(env_name)
        if v:
            return v
    return None


def upload_model_to_hub(
    model_repo: str,
    model_dir: Path,
    token: str,
    private: bool,
    dataset_repo: str,
    base_model: str,
    dry_run: bool,
):
    _require_hf_libs()
    from huggingface_hub import HfApi

    try:
        from huggingface_hub import upload_folder  # type: ignore
    except Exception:
        upload_folder = None  # type: ignore


    api = HfApi(token=token)

    if dry_run:
        print(f"[DRY RUN] Would create model repo: {model_repo} (private={private})")
    else:
        api.create_repo(repo_id=model_repo, repo_type="model", private=private, exist_ok=True)

    # Upload model folder
    print(f"[MODEL] Uploading folder: {model_dir} -> {model_repo}")
    if dry_run:
        # list a few files for visibility
        files = sorted([p.relative_to(model_dir).as_posix() for p in model_dir.rglob("*") if p.is_file()])
        print(f"[DRY RUN] {len(files)} files. First 20:")
        for f in files[:20]:
            print("  -", f)
    else:
        if upload_folder is not None:
            upload_folder(
                repo_id=model_repo,
                folder_path=str(model_dir),
                repo_type="model",
                token=token,
                commit_message="Upload trained merged model",
            )
        else:
            # Fallback: –µ—Å–ª–∏ upload_folder –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ –≤–∞—à–µ–π –≤–µ—Ä—Å–∏–∏ huggingface_hub,
            # –≥—Ä—É–∑–∏–º —Ñ–∞–π–ª—ã –ø–æ –æ–¥–Ω–æ–º—É.
            for fp in sorted(model_dir.rglob("*")):
                if not fp.is_file():
                    continue
                rel = fp.relative_to(model_dir).as_posix()
                api.upload_file(
                    path_or_fileobj=str(fp),
                    path_in_repo=rel,
                    repo_id=model_repo,
                    repo_type="model",
                    token=token,
                    commit_message=f"Upload {rel}",
                )

    # Upload/overwrite README.md
    card_text = _make_model_card(
        model_repo=model_repo,
        base_model=base_model,
        dataset_repo=dataset_repo,
    )

    if dry_run:
        print("[DRY RUN] Would upload model README.md")
    else:
        tmp = Path(".") / "_tmp_model_README.md"
        tmp.write_text(card_text, encoding="utf-8")
        api.upload_file(
            path_or_fileobj=str(tmp),
            path_in_repo="README.md",
            repo_id=model_repo,
            repo_type="model",
            token=token,
            commit_message="Add/Update model card",
        )
        tmp.unlink(missing_ok=True)

    print(f"[MODEL] Done: https://huggingface.co/{model_repo}")


def upload_datasets_to_hub(
    dataset_repo: str,
    file_infos: List[DatasetFileInfo],
    token: str,
    private: bool,
    dry_run: bool,
):
    _require_hf_libs()
    from huggingface_hub import HfApi

    api = HfApi(token=token)

    if dry_run:
        print(f"[DRY RUN] Would create dataset repo: {dataset_repo} (private={private})")
    else:
        api.create_repo(repo_id=dataset_repo, repo_type="dataset", private=private, exist_ok=True)

    # Upload dataset files under kind/filename
    print(f"[DATASET] Uploading {len(file_infos)} files -> {dataset_repo}")
    for fi in file_infos:
        path_in_repo = f"{fi.kind}/{fi.path.name}"
        if dry_run:
            print(f"[DRY RUN] Would upload {fi.path} -> {path_in_repo}")
            continue
        api.upload_file(
            path_or_fileobj=str(fi.path),
            path_in_repo=path_in_repo,
            repo_id=dataset_repo,
            repo_type="dataset",
            token=token,
            commit_message=f"Upload {fi.kind} file {fi.path.name}",
        )

    # Upload dataset index
    ds_index = _make_dataset_index(file_infos)
    if dry_run:
        print("[DRY RUN] Would upload dataset_index.json and README.md")
    else:
        tmp_idx = Path(".") / "_tmp_dataset_index.json"
        tmp_idx.write_text(json.dumps(ds_index, ensure_ascii=False, indent=2), encoding="utf-8")
        api.upload_file(
            path_or_fileobj=str(tmp_idx),
            path_in_repo="dataset_index.json",
            repo_id=dataset_repo,
            repo_type="dataset",
            token=token,
            commit_message="Add dataset index",
        )
        tmp_idx.unlink(missing_ok=True)

        # Upload README.md
        card_text = _make_dataset_card(dataset_repo=dataset_repo, file_infos=file_infos)
        tmp = Path(".") / "_tmp_dataset_README.md"
        tmp.write_text(card_text, encoding="utf-8")
        api.upload_file(
            path_or_fileobj=str(tmp),
            path_in_repo="README.md",
            repo_id=dataset_repo,
            repo_type="dataset",
            token=token,
            commit_message="Add/Update dataset card",
        )
        tmp.unlink(missing_ok=True)

    print(f"[DATASET] Done: https://huggingface.co/datasets/{dataset_repo}")


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Publish model + datasets to Hugging Face Hub")
    p.add_argument("--model_repo", type=str, required=False, help="HF repo_id for model, e.g. username/my-model")
    p.add_argument("--dataset_repo", type=str, required=False, help="HF repo_id for dataset, e.g. username/my-datasets")

    p.add_argument("--model_dir", type=str, default=None, help="Path to merged model dir. If omitted, auto-detect.")
    p.add_argument("--data_dir", type=str, default="data", help="Directory with generated jsonl datasets (default: data)")
    p.add_argument("--results_dir", type=str, default="results", help="Directory with trained_model.json (default: results)")

    p.add_argument("--include_train", action="store_true", help="Also upload data/train_*.jsonl")
    p.add_argument("--private", action="store_true", help="Create repos as private")
    p.add_argument("--token", type=str, default=None, help="HF token (or use env HF_TOKEN / huggingface-cli login)")
    p.add_argument("--base_model", type=str, default="Qwen/Qwen2.5-1.5B-Instruct", help="Base model name for README")
    p.add_argument("--dry_run", action="store_true", help="Do not upload, only print what would be done")
    return p.parse_args()


def main() -> int:
    args = parse_args()

    token = _get_token(args.token)
    if not token and not args.dry_run:
        print(
            "[WARN] HF —Ç–æ–∫–µ–Ω –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω —á–µ—Ä–µ–∑ --token/ENV. "
            "–ü–æ–ø—Ä–æ–±—É—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω –∏–∑ `huggingface-cli login` (–µ—Å–ª–∏ –≤—ã –∑–∞–ª–æ–≥–∏–Ω–µ–Ω—ã).",
            file=sys.stderr,
        )

    model_dir = discover_model_dir(args.model_dir, results_dir=args.results_dir)
   

    print("[INFO] Model dir:", model_dir)

    upload_model_to_hub(
        model_repo=args.model_repo,
        model_dir=model_dir,
        token=token,
        private=bool(args.private),
        dataset_repo=args.dataset_repo,
        base_model=args.base_model,
        dry_run=bool(args.dry_run),
    )

    file_infos = discover_dataset_files(args.data_dir, include_train=args.include_train)
    if file_infos:
        print("[INFO] Dataset files:")
        for fi in file_infos:
            print(f"  - {fi.kind:5s} {fi.path.name}")

        # Upload datasets first (—á—Ç–æ–±—ã —Å—Å—ã–ª–∫–∞ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç –±—ã–ª–∞ –≤ model card)
        upload_datasets_to_hub(
            dataset_repo=args.dataset_repo,
            file_infos=file_infos,
            token=token,
            private=bool(args.private),
            dry_run=bool(args.dry_run),
        )

    print("[OK] All done.")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
