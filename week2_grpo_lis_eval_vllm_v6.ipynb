{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec08330",
   "metadata": {},
   "source": [
    "# Week 2 — GRPO LIS: eval (vLLM)\n",
    "\n",
    "Этот ноутбук:\n",
    "- загружает **фиксированные** тестовые наборы (easy/medium/hard) из `data/*.jsonl`\n",
    "- считает accuracy для:\n",
    "  - **baseline** `Qwen/Qwen2.5-1.5B-Instruct`\n",
    "  - **trained** (путь берём из `results/trained_model.json`)\n",
    "- сохраняет числа в `results/baseline_scores.json` и `results/trained_scores.json`\n",
    "- строит **парные бары** (baseline vs trained) как в примере matplotlib.\n",
    "\n",
    "**Важно:** метрики считаются **с тем же `SYSTEM_PROMPT`**, что и в обучении, и инференс делается через **vLLM**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adb98569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если нужно — установи зависимости в eval-окружении\n",
    "# !pip install -U vllm transformers datasets accelerate matplotlib\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# (опционально) приглушить сетевые warning'и (urllib3)\n",
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e64750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_DIR: /home/yaros/DS-Mag/AI-SelectedTopics/W2-1\n",
      "DATA_DIR   : /home/yaros/DS-Mag/AI-SelectedTopics/W2-1/data\n",
      "RESULTS_DIR: /home/yaros/DS-Mag/AI-SelectedTopics/W2-1/results\n",
      "BASE_MODEL : Qwen/Qwen2.5-1.5B-Instruct\n",
      "TRAINED_MODEL_DIR: models/qwen2p5_1p5b_grpo_lis_merged\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1) Пути + модели\n",
    "# --------------------------\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_DIR = Path(\".\").resolve()\n",
    "if str(PROJECT_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_DIR))\n",
    "\n",
    "DATA_DIR = PROJECT_DIR / \"data\"\n",
    "RESULTS_DIR = PROJECT_DIR / \"results\"\n",
    "\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "# SYSTEM_PROMPT должен совпадать с train-ноутбуком (и хранится в w2_utils.py)\n",
    "from w2_utils import SYSTEM_PROMPT\n",
    "\n",
    "#CUSTOM_SYSTEM_PROMPT = SYSTEM_PROMPT\n",
    "CUSTOM_SYSTEM_PROMPT = \"\"\"Отвечай в следующем формате:\n",
    "<think>\n",
    "...\n",
    "</think>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "Output ONLY the integer in <answer> and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "# trained_model.json создаётся train-ноутбуком: {\"trained_model_dir\": \"...\"}\n",
    "trained_model_json = RESULTS_DIR / \"trained_model.json\"\n",
    "if not trained_model_json.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Не найден {trained_model_json}. Запусти train-ноутбук или создай results/trained_model.json вручную.\"\n",
    "    )\n",
    "\n",
    "TRAINED_MODEL_DIR = json.loads(trained_model_json.read_text(encoding=\"utf-8\"))[\"trained_model_dir\"]\n",
    "TRAINED_MODEL_DIR = str(TRAINED_MODEL_DIR)\n",
    "\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
    "print(\"DATA_DIR   :\", DATA_DIR)\n",
    "print(\"RESULTS_DIR:\", RESULTS_DIR)\n",
    "print(\"BASE_MODEL :\", BASE_MODEL)\n",
    "print(\"TRAINED_MODEL_DIR:\", TRAINED_MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bba7697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded testsets: {'easy': 200, 'medium': 200, 'hard': 200}\n",
      "eval_max_new_tokens = 64\n",
      "eval_temperature    = 0.0\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 2) Загрузка фиксированных testsets (jsonl)\n",
    "#    В train-ноутбуке строки сохранялись как {\"prompt\": ..., \"answer\": ...}\n",
    "# --------------------------\n",
    "from w2_utils import load_jsonl, get_user_prompt\n",
    "\n",
    "# Подхватываем спецификацию тестов из results/config.json, если она есть\n",
    "config_json = RESULTS_DIR / \"config.json\"\n",
    "if config_json.exists():\n",
    "    cfg = json.loads(config_json.read_text(encoding=\"utf-8\"))\n",
    "    test_specs = cfg.get(\"test_specs\", [])\n",
    "    eval_max_new_tokens = int(cfg.get(\"eval_max_new_tokens\", 64))\n",
    "    eval_temperature = float(cfg.get(\"eval_temperature\", 0.0))\n",
    "else:\n",
    "    cfg = {}\n",
    "    test_specs = []\n",
    "    eval_max_new_tokens = 64\n",
    "    eval_temperature = 0.0\n",
    "\n",
    "# fallback если config.json не найден или без test_specs\n",
    "if not test_specs:\n",
    "    test_specs = [\n",
    "        {\"name\": \"easy\",   \"difficulty\": 2, \"n\": 200, \"seed\": 1001},\n",
    "        {\"name\": \"medium\", \"difficulty\": 5, \"n\": 200, \"seed\": 2001},\n",
    "        {\"name\": \"hard\",   \"difficulty\": 8, \"n\": 200, \"seed\": 3001},\n",
    "    ]\n",
    "\n",
    "testsets = {}\n",
    "for spec in test_specs:\n",
    "    name = spec[\"name\"]\n",
    "    p = DATA_DIR / f\"test_{name}_d{spec['difficulty']}_n{spec['n']}_seed{spec['seed']}.jsonl\"\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Не найден тестовый датасет: {p}\\n\"\n",
    "            f\"Убедись, что train-ноутбук создал фиксированные тесты в data/.\"\n",
    "        )\n",
    "    rows = load_jsonl(p)\n",
    "    # sanity check\n",
    "    _ = get_user_prompt(rows[0])\n",
    "    _ = rows[0][\"answer\"]\n",
    "    testsets[name] = Dataset.from_list(rows)\n",
    "\n",
    "print(\"Loaded testsets:\", {k: len(v) for k, v in testsets.items()})\n",
    "print(\"eval_max_new_tokens =\", eval_max_new_tokens)\n",
    "print(\"eval_temperature    =\", eval_temperature)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a692aa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 3) Prompt builder + extractor\n",
    "# --------------------------\n",
    "from w2_utils import build_chat_prompt, extract_int\n",
    "\n",
    "# Для отладочной статистики (есть ли <answer> тег в ответе)\n",
    "ANSWER_RE = re.compile(r\"<answer>.*<\\/answer>\", re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b822247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 4) vLLM: один LLM на модель + оценка на всех датасетах\n",
    "# --------------------------\n",
    "from w2_utils import eval_all_testsets_with_llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2fd8b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-28 17:57:34 [utils.py:223] non-default args: {'tokenizer': 'Qwen/Qwen2.5-1.5B-Instruct', 'trust_remote_code': True, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'Qwen/Qwen2.5-1.5B-Instruct'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-28 17:57:36 [model.py:529] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 02-28 17:57:36 [model.py:1549] Using max model len 32768\n",
      "INFO 02-28 17:57:36 [scheduler.py:224] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 02-28 17:57:36 [vllm.py:689] Asynchronous scheduling is enabled.\n",
      "WARNING 02-28 17:57:36 [vllm.py:727] Enforce eager set, overriding optimization level to -O0\n",
      "INFO 02-28 17:57:36 [vllm.py:845] Cudagraph is disabled under eager mode\n",
      "WARNING 02-28 17:57:37 [interface.py:456] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "WARNING 02-28 17:57:37 [system_utils.py:140] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: WSL is detected and NVML is not compatible with fork\n",
      "(EngineCore_DP0 pid=25991) INFO 02-28 17:57:40 [core.py:97] Initializing a V1 LLM engine (v0.16.0) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [16384], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False, 'fuse_act_padding': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': False}, 'local_cache_dir': None, 'fast_moe_cold_start': True, 'static_all_moe_layers': []}\n",
      "(EngineCore_DP0 pid=25991) INFO 02-28 17:57:42 [parallel_state.py:1234] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.22.107.93:47805 backend=nccl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W228 17:57:52.748554940 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(EngineCore_DP0 pid=25991) INFO 02-28 17:57:52 [parallel_state.py:1445] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A\n",
      "(EngineCore_DP0 pid=25991) WARNING 02-28 17:57:52 [interface.py:456] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "(EngineCore_DP0 pid=25991) INFO 02-28 17:57:52 [gpu_model_runner.py:4124] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(EngineCore_DP0 pid=25991) /home/yaros/DS-Mag/AI-SelectedTopics/W2-1/.eval-env/lib/python3.12/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:181: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "(EngineCore_DP0 pid=25991) We recommend installing via `pip install torch-c-dlpack-ext`\n",
      "(EngineCore_DP0 pid=25991)   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(EngineCore_DP0 pid=25991) INFO 02-28 17:57:52 [cuda.py:367] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION'].\n",
      "(EngineCore_DP0 pid=25991) INFO 02-28 17:57:54 [weight_utils.py:579] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.34it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.34it/s]\n",
      "(EngineCore_DP0 pid=25991) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(EngineCore_DP0 pid=25991) INFO 02-28 17:57:54 [default_loader.py:293] Loading weights took 0.51 seconds\n",
      "(EngineCore_DP0 pid=25991) INFO 02-28 17:57:55 [gpu_model_runner.py:4221] Model loading took 2.89 GiB memory and 2.280300 seconds\n",
      "(EngineCore_DP0 pid=25991) INFO 02-28 17:57:56 [gpu_worker.py:373] Available KV cache memory: 20.2 GiB\n",
      "(EngineCore_DP0 pid=25991) INFO 02-28 17:57:56 [kv_cache_utils.py:1307] GPU KV cache size: 756,304 tokens\n",
      "(EngineCore_DP0 pid=25991) INFO 02-28 17:57:56 [kv_cache_utils.py:1312] Maximum concurrency for 32,768 tokens per request: 23.08x\n",
      "(EngineCore_DP0 pid=25991) INFO 02-28 17:57:56 [kernel_warmup.py:44] Skipping FlashInfer autotune because it is disabled.\n",
      "(EngineCore_DP0 pid=25991) INFO 02-28 17:57:56 [core.py:278] init engine (profile, create kv cache, warmup model) took 1.52 seconds\n",
      "(EngineCore_DP0 pid=25991) INFO 02-28 17:57:58 [vllm.py:689] Asynchronous scheduling is enabled.\n",
      "(EngineCore_DP0 pid=25991) WARNING 02-28 17:57:58 [vllm.py:734] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.\n",
      "(EngineCore_DP0 pid=25991) INFO 02-28 17:57:58 [vllm.py:845] Cudagraph is disabled under eager mode\n",
      "INFO 02-28 17:57:58 [llm.py:355] Supported tasks: ['generate']\n",
      "\n",
      "=== BASELINE ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5912c7dd6cf9457f8e22e5978ad3de0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfc78810cfd463c842d4bf00d53b280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[easy] acc=0.0000 (n=200)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3f893b3ff940c78f2d00adb75f3d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce3a4b706f54c39ac4f956597ddb29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[medium] acc=0.0000 (n=200)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7326b842ff440d92e299e41441524d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3d061bcd29409f8c0f8fda501603d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hard] acc=0.0050 (n=200)\n",
      "INFO 02-28 17:58:00 [utils.py:223] non-default args: {'tokenizer': 'Qwen/Qwen2.5-1.5B-Instruct', 'trust_remote_code': True, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'models/qwen2p5_1p5b_grpo_lis_merged'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-28 17:58:00 [model.py:529] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 02-28 17:58:00 [model.py:1549] Using max model len 32768\n",
      "INFO 02-28 17:58:00 [scheduler.py:224] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 02-28 17:58:00 [vllm.py:727] Enforce eager set, overriding optimization level to -O0\n",
      "INFO 02-28 17:58:00 [vllm.py:845] Cudagraph is disabled under eager mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(EngineCore_DP0 pid=26319) INFO 02-28 17:58:04 [core.py:97] Initializing a V1 LLM engine (v0.16.0) with config: model='models/qwen2p5_1p5b_grpo_lis_merged', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=models/qwen2p5_1p5b_grpo_lis_merged, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [16384], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False, 'fuse_act_padding': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': False}, 'local_cache_dir': None, 'fast_moe_cold_start': True, 'static_all_moe_layers': []}\n",
      "(EngineCore_DP0 pid=26319) INFO 02-28 17:58:05 [parallel_state.py:1234] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.22.107.93:47927 backend=nccl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W228 17:58:15.937921627 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(EngineCore_DP0 pid=26319) INFO 02-28 17:58:15 [parallel_state.py:1445] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A\n",
      "(EngineCore_DP0 pid=26319) WARNING 02-28 17:58:15 [interface.py:456] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "(EngineCore_DP0 pid=26319) INFO 02-28 17:58:15 [gpu_model_runner.py:4124] Starting to load model models/qwen2p5_1p5b_grpo_lis_merged...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(EngineCore_DP0 pid=26319) /home/yaros/DS-Mag/AI-SelectedTopics/W2-1/.eval-env/lib/python3.12/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:181: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "(EngineCore_DP0 pid=26319) We recommend installing via `pip install torch-c-dlpack-ext`\n",
      "(EngineCore_DP0 pid=26319)   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(EngineCore_DP0 pid=26319) INFO 02-28 17:58:16 [cuda.py:367] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION'].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]\n",
      "(EngineCore_DP0 pid=26319) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(EngineCore_DP0 pid=26319) INFO 02-28 17:58:16 [default_loader.py:293] Loading weights took 0.52 seconds\n",
      "(EngineCore_DP0 pid=26319) INFO 02-28 17:58:17 [gpu_model_runner.py:4221] Model loading took 2.89 GiB memory and 0.857862 seconds\n",
      "(EngineCore_DP0 pid=26319) INFO 02-28 17:58:18 [gpu_worker.py:373] Available KV cache memory: 20.2 GiB\n",
      "(EngineCore_DP0 pid=26319) INFO 02-28 17:58:18 [kv_cache_utils.py:1307] GPU KV cache size: 756,304 tokens\n",
      "(EngineCore_DP0 pid=26319) INFO 02-28 17:58:18 [kv_cache_utils.py:1312] Maximum concurrency for 32,768 tokens per request: 23.08x\n",
      "(EngineCore_DP0 pid=26319) INFO 02-28 17:58:18 [kernel_warmup.py:44] Skipping FlashInfer autotune because it is disabled.\n",
      "(EngineCore_DP0 pid=26319) INFO 02-28 17:58:18 [core.py:278] init engine (profile, create kv cache, warmup model) took 1.54 seconds\n",
      "(EngineCore_DP0 pid=26319) INFO 02-28 17:58:20 [vllm.py:689] Asynchronous scheduling is enabled.\n",
      "(EngineCore_DP0 pid=26319) WARNING 02-28 17:58:20 [vllm.py:734] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.\n",
      "(EngineCore_DP0 pid=26319) INFO 02-28 17:58:20 [vllm.py:845] Cudagraph is disabled under eager mode\n",
      "INFO 02-28 17:58:21 [llm.py:355] Supported tasks: ['generate']\n",
      "\n",
      "=== TRAINED ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f68ae5d93f4022899d7c3d0845d1c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81679402dac8494bbbfc413e511d55e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[easy] acc=0.3650 (n=200)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5bdc53b00541aca9bbfaaa326da046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371b6a0157354b09baf3d06adbd83184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[medium] acc=0.1350 (n=200)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4cd73cc9db4494b92238c6b9803cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09b3b6512a14591b0efee510187ad16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hard] acc=0.0400 (n=200)\n",
      "\n",
      "Saved:\n",
      " - /home/yaros/DS-Mag/AI-SelectedTopics/W2-1/results/baseline_scores.json\n",
      " - /home/yaros/DS-Mag/AI-SelectedTopics/W2-1/results/trained_scores.json\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 5) Запуск eval: baseline vs trained\n",
    "# --------------------------\n",
    "# tokenizer используем от BASE_MODEL (после RL он обычно не меняется)\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "gpu_mem_util = 0.3\n",
    "# baseline LLM\n",
    "baseline_llm = LLM(\n",
    "    model=BASE_MODEL,\n",
    "    tokenizer=BASE_MODEL,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"auto\",\n",
    "    enforce_eager=True,\n",
    "    gpu_memory_utilization=gpu_mem_util\n",
    ")\n",
    "\n",
    "print(\"\\n=== BASELINE ===\")\n",
    "baseline_scores = eval_all_testsets_with_llm(\n",
    "    baseline_llm, tok, testsets,\n",
    "    system_prompt=CUSTOM_SYSTEM_PROMPT,\n",
    "    max_new_tokens=eval_max_new_tokens,\n",
    "    temperature=eval_temperature,\n",
    ")\n",
    "\n",
    "# Удаляем baseline движок перед созданием trained\n",
    "# del baseline_llm\n",
    "# import gc, torch, time\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# time.sleep(2)\n",
    "\n",
    "# trained LLM (merged dir)\n",
    "trained_llm = LLM(\n",
    "    model=TRAINED_MODEL_DIR,\n",
    "    tokenizer=BASE_MODEL,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"auto\",\n",
    "    enforce_eager=True,\n",
    "    gpu_memory_utilization=gpu_mem_util\n",
    ")\n",
    "\n",
    "print(\"\\n=== TRAINED ===\")\n",
    "trained_scores = eval_all_testsets_with_llm(\n",
    "    trained_llm, tok, testsets,\n",
    "    system_prompt=CUSTOM_SYSTEM_PROMPT,\n",
    "    max_new_tokens=eval_max_new_tokens,\n",
    "    temperature=eval_temperature,\n",
    ")\n",
    "\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(RESULTS_DIR / \"baseline_scores.json\").write_text(json.dumps(baseline_scores, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "(RESULTS_DIR / \"trained_scores.json\").write_text(json.dumps(trained_scores, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" -\", RESULTS_DIR / \"baseline_scores.json\")\n",
    "print(\" -\", RESULTS_DIR / \"trained_scores.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "123879ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary ===\n",
      "    easy: baseline=0.0000 | trained=0.3650 | delta=+0.3650\n",
      "  medium: baseline=0.0000 | trained=0.1350 | delta=+0.1350\n",
      "    hard: baseline=0.0050 | trained=0.0400 | delta=+0.0350\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 6) Числа + дельты (trained - baseline)\n",
    "# --------------------------\n",
    "names = list(testsets.keys())\n",
    "print(\"\\n=== Summary ===\")\n",
    "for n in names:\n",
    "    b = baseline_scores[n]\n",
    "    t = trained_scores[n]\n",
    "    print(f\"{n:>8}: baseline={b:.4f} | trained={t:.4f} | delta={t-b:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb2ae01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testset sizes: {'easy': 200, 'medium': 200, 'hard': 200}\n",
      "\n",
      "=== easy sample ===\n",
      "PROMPT (head): You are given a sequence of integers.  Task: compute the length of the Longest Increasing Subsequence (LIS).  Definitions: - A subsequence is obtained by deleting zero or more elements without changin\n",
      "GOLD: 4\n",
      "\n",
      "=== medium sample ===\n",
      "PROMPT (head): You are given a sequence of integers.  Task: compute the length of the Longest Increasing Subsequence (LIS).  Definitions: - A subsequence is obtained by deleting zero or more elements without changin\n",
      "GOLD: 5\n",
      "\n",
      "=== hard sample ===\n",
      "PROMPT (head): You are given a sequence of integers.  Task: compute the length of the Longest Increasing Subsequence (LIS).  Definitions: - A subsequence is obtained by deleting zero or more elements without changin\n",
      "GOLD: 7\n",
      "\n",
      "===== DEBUG baseline =====\n",
      "\n",
      "--- dataset easy (k=3) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d9ec0922dc4540b30f0b3fc9d9ddb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204884a6754544968daf22b37a2bb8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=4 pred=None ok=False\n",
      "out_tail: ython def length_of_LIS(sequence):     if not sequence:         return 0          dp = [1] * len(sequence)          for i in range(1, len(sequence)):         for j in range(i):             if sequence[i] > sequence[j]:                 dp[i] = max(dp[i], dp[j] + 1)          return max(dp)  # Example usage sequence = [-8\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebcdbcabc314eb4a5b92fe31a888564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bea71ea0e7448c3abffb60ab2cf5635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=3 pred=4 ok=False\n",
      "out_tail: y `dp` where `dp[i]` represents the length of the LIS ending with the element at index `i`. We will iterate through the sequence and update the `dp` array accordingly. The length of the LIS will be the maximum value in the `dp` array. </think>  <answer> - The length of the Longest Increasing Subsequence is 4. </answer>\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569d5ade78154564a32e52a5936bbc7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06bbb0f2f40f4615b1b83620258af1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=4 pred=10 ok=False\n",
      "out_tail: nt at index `i`. We will iterate through the sequence and update the `dp` array accordingly. The maximum value in the `dp` array will give us the length of the LIS. </think>  <answer> [1, 2, 2, 2, 1, 2, 2, 2, 1, 2] </answer>  The LIS of the given sequence is [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]. The length of this LIS is 10.\n",
      "answer_tag: None\n",
      "subset_acc=0.000 | parsed_rate=0.667 | has_answer_tag_rate=0.000\n",
      "\n",
      "--- dataset medium (k=3) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efef12465f14d6984a2e941167dd14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46253578853e4ee4a207c078cc2e0cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=5 pred=19 ok=False\n",
      "out_tail:  `dp[i]` represents the length of the LIS ending at index `i`. We iterate through the sequence and update the `dp` array accordingly. The maximum value in the `dp` array will give us the length of the LIS. </think>  <answer> -19, 9, 15, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19 </answer>  The LIS length is 19.\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd884fed4c2e4541a27013eb218ca91c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d4806de6d748efa0e1732526cd58de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=6 pred=2 ok=False\n",
      "out_tail: ray `dp` where `dp[i]` represents the length of the LIS ending at index `i`. We iterate through the sequence and update the `dp` array accordingly. The maximum value in the `dp` array will give us the length of the LIS. </think>  <answer> -20, 20, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2 </answer>  The LIS length is 2.\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bf4d399cdc46d0af731bf1ff2b4abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac80f145b3dc478cbb01f1d2a7b121be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=6 pred=-11 ok=False\n",
      "out_tail: quence (LIS), we can use dynamic programming. We will maintain an array `dp` where `dp[i]` represents the length of the LIS ending at index `i`. We iterate through the sequence and update the `dp` array accordingly. The maximum value in the `dp` array will give us the length of the LIS. </think>  <answer> -11 </answer>\n",
      "answer_tag: None\n",
      "subset_acc=0.000 | parsed_rate=1.000 | has_answer_tag_rate=0.000\n",
      "\n",
      "--- dataset hard (k=3) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635d319e70984225906f7100fc25bd44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac79789fd64406c990a37dee9bfe4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=7 pred=None ok=False\n",
      "out_tail: p` where `dp[i]` represents the length of the LIS ending with the element at index `i`. We will iterate through the sequence and update the `dp` array accordingly. The length of the LIS will be the maximum value in the `dp` array. </think>  <answer> [2, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5] </answer>\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c184b2e6b545abbb8a15b3eb8c40af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15c4cba3e804808966db56388dcdd32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=7 pred=None ok=False\n",
      "out_tail: re `dp[i]` represents the length of the LIS ending with the element at index `i`. We will iterate through the sequence and update the `dp` array accordingly. The length of the LIS will be the maximum value in the `dp` array. </think>  <answer> [1, 1, 2, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7] </answer>\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb77740b41d48a99d2b2b4d4d34d426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c823b658b49d44639d49d31af34ca3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=7 pred=None ok=False\n",
      "out_tail: he length of the LIS ending with the element at index `i`. We will iterate through the sequence and update the `dp` array accordingly. The length of the LIS will be the maximum value in the `dp` array. </think>  <answer> -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 </answer>  The length of the LIS is 11.\n",
      "answer_tag: None\n",
      "subset_acc=0.000 | parsed_rate=0.000 | has_answer_tag_rate=0.000\n",
      "\n",
      "===== DEBUG trained =====\n",
      "\n",
      "--- dataset easy (k=3) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9987d783436480bb15ec1070fb0cf94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea85b33b9b747848685d05cecf99b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=4 pred=3 ok=False\n",
      "out_tail: <answer> 3 </answer> <think> To find the length of the Longest Increasing Subsequence (LIS), we need to identify the longest subsequence of the given sequence where each element is greater than the previous one. We can use dynamic programming to solve this problem efficiently. </think>\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34df65a3cb6474aaa74ee950e936f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c37f4198c64a8fb23f03799278ee22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=3 pred=4 ok=False\n",
      "out_tail: <answer> 4 </answer> <think> To find the length of the Longest Increasing Subsequence (LIS), we need to identify the longest subsequence of the given sequence where each element is strictly greater than the previous one. We can use dynamic programming to solve this problem efficiently. </think>\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c21129b02d450f95e283ea054d9683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9981ce68f4b948c6863d9eecca7171cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=4 pred=4 ok=True\n",
      "out_tail: <answer> 4 </answer> <think> To find the length of the Longest Increasing Subsequence (LIS), we need to identify the longest subsequence of the given sequence where each element is greater than the previous one. We can use dynamic programming to solve this problem efficiently. </think>\n",
      "answer_tag: None\n",
      "subset_acc=0.333 | parsed_rate=1.000 | has_answer_tag_rate=0.000\n",
      "\n",
      "--- dataset medium (k=3) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5cdd22e9ef646d68ecfa06fbf7d18f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765db119240d49f2a359857a213b393b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=5 pred=4 ok=False\n",
      "out_tail: <answer> 4 </answer>\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806f4acba16b437abf47e6feddfcbd10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0e51d6b6f846b49f9dd700e87896d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=6 pred=4 ok=False\n",
      "out_tail: <answer> 4 </answer>\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232960a2e83b489298c3c152c7124458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2305994684514051bafe7898a8e6332c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=6 pred=4 ok=False\n",
      "out_tail: LIS), we can use dynamic programming. We will maintain an array `dp` where `dp[i]` represents the length of the LIS ending at index `i`. We iterate through the sequence and for each element, we check all previous elements to see if they can be part of the LIS. If they can, we update the `dp` value accordingly. </think>\n",
      "answer_tag: None\n",
      "subset_acc=0.000 | parsed_rate=1.000 | has_answer_tag_rate=0.000\n",
      "\n",
      "--- dataset hard (k=3) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acee26db7f8e4bf0be7202ad72abe993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8b7a098b2c404586eafd9ec265c65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=7 pred=4 ok=False\n",
      "out_tail: <answer> 4 </answer> <think> To find the length of the Longest Increasing Subsequence (LIS), we need to identify the longest subsequence of the given sequence where each element is strictly greater than the previous one. We can use dynamic programming to solve this problem efficiently. </think>\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376a8dcef89a4756ae1c0d9245cbe291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbddec66e994cd2b2453a00b3b73678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=7 pred=4 ok=False\n",
      "out_tail: <answer> 4 </answer> <think> To find the length of the Longest Increasing Subsequence (LIS), we need to identify the longest subsequence of the given sequence where each element is strictly greater than the previous one. We can use dynamic programming to solve this problem efficiently. </think>\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2ba590c1f54653b9ed94adfa519e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d55ead0fbe40ecaded3d2073f12f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=7 pred=5 ok=False\n",
      "out_tail: n use dynamic programming. We will maintain an array `dp` where `dp[i]` represents the length of the LIS ending at index `i`. We iterate through the sequence and for each element, we check all previous elements to see if they can be part of an increasing subsequence. If they can, we update `dp[i]` accordingly. </think>\n",
      "answer_tag: None\n",
      "subset_acc=0.000 | parsed_rate=1.000 | has_answer_tag_rate=0.000\n",
      "\n",
      "=== TRAINED on saved dev_* (sanity) ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381b8f0bf2bf43c6ad11f59a410af67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66688ae0efb402fb84fd0887dbc57f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dev_easy_d2_n128_seed4001] acc=0.3594 (n=128)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d244035de94d0c807e6b974c20cb92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b3d17ae2a745b98683227fec0c9fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dev_hard_d8_n128_seed4003] acc=0.0703 (n=128)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7120c0ea5c0a4f6893bac67f603b36e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741aa2e6a21a4498ac20deeeca20aa91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dev_medium_d5_n128_seed4002] acc=0.1250 (n=128)\n",
      "dev_easy_d2_n128_seed4001: acc=0.3594 (n=128)\n",
      "dev_hard_d8_n128_seed4003: acc=0.0703 (n=128)\n",
      "dev_medium_d5_n128_seed4002: acc=0.1250 (n=128)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 7) DEBUG / sanity-check generation & parsing\n",
    "# --------------------------\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "DEBUG_K = 3\n",
    "DEBUG_MAX_NEW_TOKENS = 256\n",
    "\n",
    "def _preview_examples(ds, n: int = 1) -> None:\n",
    "    n = int(n)\n",
    "    for i in range(min(n, len(ds))):\n",
    "        ex = ds[i]\n",
    "        p = get_user_prompt(ex)\n",
    "        print(\"PROMPT (head):\", p[:200].replace(\"\\n\", \" \"))\n",
    "        print(\"GOLD:\", ex.get(\"answer\"))\n",
    "\n",
    "print(\"Testset sizes:\", {k: len(v) for k, v in testsets.items()})\n",
    "for name, ds in testsets.items():\n",
    "    print(f\"\\n=== {name} sample ===\")\n",
    "    _preview_examples(ds, n=1)\n",
    "\n",
    "def debug_generate_one(llm, ex: Dict[str, Any], max_new_tokens: int = 128) -> Tuple[Optional[int], str]:\n",
    "    user_prompt = get_user_prompt(ex)\n",
    "    prompt_text = build_chat_prompt(tok, user_prompt)\n",
    "\n",
    "    params = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        max_tokens=int(max_new_tokens),\n",
    "        #stop=[\"</answer>\"],  # same as eval (can remove stop if you want)\n",
    "    )\n",
    "    out_text = llm.generate([prompt_text], params)[0].outputs[0].text\n",
    "    pred = extract_int(out_text)\n",
    "    with open(\"log.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(out_text + \"\\n\")\n",
    "        f.write(\"pred=\"+ str(pred) + \"\\n\")\n",
    "    return pred, out_text\n",
    "\n",
    "def run_debug_on_subset(llm, datasets: Dict[str, Any], model_name: str) -> None:\n",
    "    print(f\"\\n===== DEBUG {model_name} =====\")\n",
    "    for ds_name, ds in datasets.items():\n",
    "        k = min(int(DEBUG_K), len(ds))\n",
    "        if k <= 0:\n",
    "            print(f\"--- dataset {ds_name}: empty ---\")\n",
    "            continue\n",
    "\n",
    "        correct = 0\n",
    "        parsed = 0\n",
    "        has_tag = 0\n",
    "\n",
    "        print(f\"\\n--- dataset {ds_name} (k={k}) ---\")\n",
    "        for i in range(k):\n",
    "            ex = ds[i]  # IMPORTANT: Dataset[:k] returns dict-of-columns -> BUG. Use index!\n",
    "\n",
    "            pred, out_text = debug_generate_one(llm, ex, max_new_tokens=DEBUG_MAX_NEW_TOKENS)\n",
    "\n",
    "            gold_raw = ex.get(\"answer\", None)\n",
    "            try:\n",
    "                gold = int(gold_raw) if gold_raw is not None else None\n",
    "            except Exception:\n",
    "                gold = None\n",
    "\n",
    "            ok = (pred is not None and gold is not None and int(pred) == int(gold))\n",
    "            correct += int(ok)\n",
    "            parsed  += int(pred is not None)\n",
    "            has_tag += int(ANSWER_RE.search(out_text) is not None)\n",
    "\n",
    "            print(f\"gold={gold} pred={pred} ok={ok}\")\n",
    "            print(\"out_tail:\", out_text[-320:].replace(\"\\n\",\" \"))\n",
    "\n",
    "            m = ANSWER_RE.search(out_text)\n",
    "            print(\"answer_tag:\", m.group(0) if m else None)\n",
    "\n",
    "        print(f\"subset_acc={correct/k:.3f} | parsed_rate={parsed/k:.3f} | has_answer_tag_rate={has_tag/k:.3f}\")\n",
    "\n",
    "# Small sanity-check on real tasks (prints a few generations)\n",
    "run_debug_on_subset(baseline_llm, testsets, \"baseline\")\n",
    "run_debug_on_subset(trained_llm,  testsets, \"trained\")\n",
    "\n",
    "# --------------------------\n",
    "# Extra sanity: evaluate TRAINED on saved dev_*.jsonl from train-notebook (if present)\n",
    "# --------------------------\n",
    "dev_files = sorted(DATA_DIR.glob(\"dev_*.jsonl\"))\n",
    "if dev_files:\n",
    "    devsets = {p.stem: Dataset.from_list(load_jsonl(p)) for p in dev_files}\n",
    "    print(\"\\n=== TRAINED on saved dev_* (sanity) ===\")\n",
    "    dev_scores = eval_all_testsets_with_llm(\n",
    "        trained_llm, tok, devsets,\n",
    "        system_prompt=CUSTOM_SYSTEM_PROMPT,\n",
    "        max_new_tokens=eval_max_new_tokens,\n",
    "        temperature=eval_temperature,\n",
    "    )\n",
    "    for name, acc in dev_scores.items():\n",
    "        print(f\"{name}: acc={acc:.4f} (n={len(devsets[name])})\")\n",
    "else:\n",
    "    print(\"\\n[INFO] No dev_*.jsonl files found in DATA_DIR. Skipping dev sanity eval.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e37295d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot: /home/yaros/DS-Mag/AI-SelectedTopics/W2-1/results/paired_bars_accuracy.png\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 7) Парные бары (baseline vs trained) — как в примере matplotlib\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/barchart.html\n",
    "# --------------------------\n",
    "from w2_utils import plot_paired_bars_accuracy\n",
    "\n",
    "out_png = RESULTS_DIR / \"paired_bars_accuracy.png\"\n",
    "plot_paired_bars_accuracy(\n",
    "    baseline_scores,\n",
    "    trained_scores,\n",
    "    out_path=out_png,\n",
    "    title=\"Accuracy: baseline vs trained\",\n",
    ")\n",
    "\n",
    "print(\"Saved plot:\", out_png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08eabd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot: /home/yaros/DS-Mag/AI-SelectedTopics/W2-1/results/paired_bars_accuracy.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 7) Парные бары (baseline vs trained) — как в примере matplotlib\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/barchart.html\n",
    "# --------------------------\n",
    "names = list(testsets.keys())\n",
    "base_vals = [baseline_scores[n] for n in names]\n",
    "tr_vals = [trained_scores[n] for n in names]\n",
    "\n",
    "x = list(range(len(names)))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "bars1 = ax.bar([i - width/2 for i in x], base_vals, width, label=\"Baseline (Qwen2.5-1.5B-Instruct)\")\n",
    "bars2 = ax.bar([i + width/2 for i in x], tr_vals, width, label=\"Trained (GRPO)\")\n",
    "\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Baseline vs Trained accuracy on fixed testsets\")\n",
    "ax.set_xticks(x, names)\n",
    "ax.set_ylim(0.0, 1.0)\n",
    "ax.legend(loc=\"best\")\n",
    "\n",
    "def autolabel(rects):\n",
    "    for r in rects:\n",
    "        h = r.get_height()\n",
    "        ax.annotate(\n",
    "            f\"{h:.3f}\",\n",
    "            xy=(r.get_x() + r.get_width() / 2, h),\n",
    "            xytext=(0, 3),\n",
    "            textcoords=\"offset points\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "autolabel(bars1)\n",
    "autolabel(bars2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plot_path = RESULTS_DIR / \"paired_bars_accuracy.png\"\n",
    "fig.savefig(plot_path, dpi=160)\n",
    "print(\"Saved plot:\", plot_path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d04173f",
   "metadata": {},
   "source": [
    "## Что вставить в отчёт\n",
    "1) Табличку/список `baseline` vs `trained` + `delta` по easy/medium/hard.  \n",
    "2) Картинку `results/paired_bars_accuracy.png`.  \n",
    "3) Пару предложений, где улучшение наиболее заметно и почему (обычно medium/hard выигрывают больше).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds-mag-ai-sel-top-w2-1-eval)",
   "language": "python",
   "name": "ds-mag-ai-sel-top-w2-1-eval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "name": "week2_grpo_lis_eval_vllm_v5"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
