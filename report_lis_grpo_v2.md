# Отчёт по задаче LIS: GRPO‑дообучение Qwen2.5‑1.5B‑Instruct

## 1) Цель

Моя цель была — дообучить **Qwen2.5‑1.5B‑Instruct** под задачу **LIS (Longest Increasing Subsequence)** так, чтобы модель:

- корректно понимала постановку и считала **длину** LIS (а не просто «рассказывала алгоритм»);
- стабильно отвечала в удобном машинно‑парсимом формате;
- показывала прирост качества по сравнению с исходной базовой моделью на наборах разной сложности (**easy / medium / hard**).

## 2) Постановка и данные

Задача: по входной последовательности чисел выдать одно целое число — **длину LIS**.

Для измерений использовались фиксированные наборы (seeds фиксированы), разделённые по сложности:

- **easy**
- **medium**
- **hard**

Это важно: из‑за фиксированных наборов я могу сравнивать модели «яблоко с яблоком» (без дрейфа датасета между запусками).

## 3) Подход к обучению

### 3.1 Общая схема

- База: **Qwen2.5‑1.5B‑Instruct**
- Обучение: **GRPO** (policy optimization) с **LoRA** (экономно по памяти/времени)
- Наблюдаем метрики на **dev** по всем трём поднаборам (easy/medium/hard), чтобы видеть баланс качества.

Ключевая идея: я не пытался «в лоб» выжать hard сразу. Сначала — научить модель *правильно отвечать* и давать хоть какой‑то корректный сигнал на простых примерах, а затем переносить на более сложные случаи.

### 3.2 Формат ответа и остановка генерации

Во время обучения модель генерирует текст, в котором есть блок ответа:

- `<answer> ... </answer>`

Я использовал остановку по `</answer>` (и/или тримминг до этого тега) — это сильно уменьшает «раздувание» ответов, ускоряет dev‑eval и снижает вероятность того, что модель сама себя “засорит” лишними числами.

### 3.3 Reward shaping

Reward был составным:

- **correctness_reward**: 1.0 если число верное, иначе 0.0  
- **format_penalty**: штраф, если не получилось извлечь целое число из ответа
- **distance_reward** (мягкий сигнал): небольшой бонус, если модель близко к правильному числу

Смысл такой комбинации:
- correctness даёт «жёсткую» цель,
- format_penalty заставляет соблюдать контракт вывода,
- distance_reward помогает, когда модель ещё “не дотягивает” до правильного числа и иначе всё было бы только 0/1.

### 3.4 Curriculum (две фазы)

Я обучал в 2 фазы:

**Фаза 1 (phase1_d1-5)**  
- сложности: `1–5`
- шагов: `200`
- dev‑eval: каждые `50` шагов (quick + full по улучшению)

Цель фазы 1: быстро поднять базовую точность и приучить модель к «короткому и проверяемому» ответу.

**Фаза 2 (phase2_d6-10)**  
- сложности: `6–10`
- шагов: `400`
- dev‑eval: каждые `20` шагов, и для фазы 2 я считал **FULL** (n=128 на каждый split)

Дополнительно, чтобы удерживать качество на medium и не “забывать” более простые паттерны:
- **replay_fraction = 0.20** из difficulty **4–5** (то есть фазу 2 я делал как микс: ~80% (6–10) + ~20% (4–5))

Это сознательный компромисс: если гнать только 6–10, hard может расти, но medium часто начинает плавать сильнее.

## 4) Как шло обучение и что именно давало прирост

### 4.1 Фаза 1: модель сначала «многословит», затем учится отвечать кратко и по делу

На старте фазы 1 (примерно step=10) по логам было видно:

- **completions / mean_length ≈ 40.9**  
- **clipped_ratio ≈ 0.536** (то есть модель регулярно упиралась в max_completion_length)
- correctness_reward ещё низкий (**≈ 0.10**)
- format_penalty заметный отрицательный (**≈ -0.064**)

Интерпретация: модель в начале часто генерировала длинные рассуждения и не всегда доходила до чистого “ответа‑числа” в правильном месте.

Дальше фаза 1 дала очень понятную динамику:
- длина генераций быстро сокращалась (модель «поймала», что нужно выдавать число быстро),
- clipped_ratio упал почти до нуля,
- reward вырос (например, на step=100 reward уже был **≈ 0.422**),
- correctness_reward на step=100 был уже **≈ 0.397**.

### 4.2 Фаза 1: dev‑метрики и «что считать лучшим»

Ключевые FULL‑оценки на фазе 1:

- step 50: dev/avg ≈ **0.3703**
- step 100: dev/avg ≈ **0.3805**
- step 200: dev/avg ≈ **0.3813** (лучшее FULL в этой фазе)

Reward и training loss показывают, как модель оптимизирует сконструированную нами целевую функцию (correctness + shaping + KL).  
Однако финальной метрикой задачи является accuracy на dev/test.

Поэтому при выборе «лучшей» точки обучения я ориентировался именно на dev accuracy, 
а не на максимум reward, поскольку reward содержит дополнительные компоненты 
(format penalty, distance shaping, KL), которые могут улучшаться без реального роста точности.

### 4.3 Фаза 2: перенос на сложные случаи и баланс “hard vs medium”

Фаза 2 стартовала с лучшего LoRA‑состояния из фазы 1 (то есть не «с нуля», а с уже «приученной» к формату и базовому сигналу модели).

Что стало заметно на фазе 2:

1) **Ответы остаются короткими**  
Средняя длина completion держалась около **10–11** токенов, что хорошо: модель не “тонет” в длинном тексте.

2) **Сложность выросла — correctness ниже, KL выше**  
По train‑логам correctness_reward в среднем ниже, чем на фазе 1 (ожидаемо), а KL заметно выше (модель сильнее отходит от ref).

3) **FULL dev на каждом eval в фазе 2 оказался полезнее quick**  
Quick‑подвыборка на сложных задачах слишком шумная. Я переключил фазу 2 на FULL‑eval, чтобы отбирать лучшие чекпоинты по реально устойчивой метрике.

FULL dev‑оценки по фазе 2 (фрагмент):

- step 240: dev/avg ≈ **0.3516**
- step 260: dev/avg ≈ **0.3688** (easy=0.4453, medium=0.3672, hard=0.3438) — **лучшее в прогоне**
- дальше метрика “дышит” и в среднем держится в районе **0.33–0.35**

4) **Replay 20% (4–5) — попытка стабилизировать medium без убийства hard**  
По ощущениям, replay действительно помогает не «просаживать» medium слишком сильно, когда я давлю на 6–10. При этом hard всё равно остаётся в разумном диапазоне (~0.30–0.35).

Вывод по фазе 2: модель переносится на сложность лучше, чем baseline, но рост метрики не монотонный — есть «пик» (step 260), после чего идёт флуктуация. Это говорит о том, что:
- либо нужен более аккуратный early stopping,
- либо нужно менять “температуру обучения” (LR/KL/num_generations/объём данных), а не просто «дольше учить».

### 4.4 Верификатор и неоднозначные ответы

В процессе я увидел, что на практике встречаются ответы вида:

`<answer> [1, 2, 2, 2, 1, 2, ...] </answer>`

Если верификатор в таких случаях «просто берёт последнюю цифру», появляется риск ложноположительных оценок и (теоретически) reward hacking.

Я ужесточил верификатор так, чтобы:
- списки/массивы внутри `<answer>` не засчитывались как валидный ответ,
- неоднозначные случаи (много чисел без явного маркера) давали `None`,
- принимались только “однозначные” числа или числа с явной маркировкой (“answer=”, “length is”, и т.п.).

При этом по моим логам **динамика обучения и итоговые dev‑метрики существенно не «переломились»** — то есть, похоже, модель не массово «читерила» этой лазейкой, а основные проблемы/успехи лежали в другом: в переносе на сложность и в стабильности оптимизации.

Тем не менее, для честности метрики и интерпретации результатов такой верификатор важен: он убирает класс ложных побед.

## 5) Eval в vLLM и почему изначально baseline мог выглядеть как “≈0”

На этапе eval я столкнулся с тем, что **baseline** иногда выглядит как почти нулевая точность, хотя интуитивно это подозрительно.

По дебагу выяснилось, что baseline часто:
- не следует `<answer>...</answer>` формату,
- пишет рассуждения и “лишние” числа,
- из‑за чего строгая процедура извлечения ответа либо не находит число, либо находит “не то” число.

Чтобы не оценивать только «насколько модель соблюдает формат», я для себя сделал два режима измерения (оба — одинаковые для baseline и trained, т.е. сравнение честное внутри режима):

### 5.1 Режим A: исходный SYSTEM_PROMPT (формат важен)

Результат:

- easy: baseline=0.0000 | trained=0.3650
- medium: baseline=0.0000 | trained=0.1350
- hard: baseline=0.0050 | trained=0.0400

Интерпретация: trained‑модель намного лучше соблюдает контракт вывода и чаще выдаёт извлекаемый ответ. Baseline почти не попадает в «узкую» рамку.

### 5.2 Режим B: тот же формат, но с явным запретом “лишних чисел”

Я добавил в системную инструкцию строку уровня:

> Output ONLY the integer in <answer> and nothing else.

Этот режим я использовал **не чтобы “подкрутить” метрики**, а чтобы посмотреть шире:  
насколько модели реально решают LIS, если минимизировать «шум» от чисел в рассуждениях и убрать неоднозначность парсинга.

Результат в этом режиме:

- easy: baseline=0.0650 | trained=0.4450
- medium: baseline=0.1700 | trained=0.3150
- hard: baseline=0.0700 | trained=0.2950

Эти числа выглядят более согласованно с тем, что я видел на dev во время обучения (например, hard уже не «0.04», а ~0.30).

**Главный вывод:** абсолютные числа сильно зависят от того, оцениваю ли я “решение LIS + соблюдение формата” или пытаюсь оценить “решение LIS” более чисто. Поэтому я оставляю оба режима как диагностику, но для отчётности важно фиксировать выбранный протокол.

## 6) Итоги, что получилось и что нет

### Что получилось

- Я получил устойчивый прирост качества по сравнению с baseline (особенно заметный в режиме, где минимизирован форматный шум).
- Фаза 1 хорошо “поставила” модели формат и базовый навык решения.
- Фаза 2 дала перенос на сложные задачи и подняла hard до адекватных значений, но с заметной нестабильностью и пиками.

### Что не получилось «идеально»

- Во второй фазе качество не растёт монотонно: есть лучший чекпоинт, после которого метрика колеблется.
- Баланс hard/medium требует аккуратной настройки (replay помогает, но не решает всё).

## 7) Что бы я делал дальше (если бы было больше времени)

- Увеличил бы объём trainset на фазе 2 и/или число генераций (G), чтобы снизить variance GRPO.
- Добавил бы явный штраф за “лишние числа” именно внутри `<answer>...</answer>`, чтобы ускорить выработку правильного поведения.
- Протестировал бы 2–3 режима curriculum для фазы 2:
  - (6–10) + replay (4–5) как сейчас,
  - (6–10) + replay (1–5) чуть шире,
  - или 3‑фазный вариант: 1–5 → 4–7 → 6–10.
- Жёстче бы автоматизировал early stopping по FULL dev/avg (паттерн «пик — затем флаттер» явно просится под остановку).

---

**Короткий итог:** GRPO+LoRA действительно смогло заметно улучшить Qwen2.5‑1.5B под LIS. Главная практическая сложность оказалась не в “чистом обучении”, а в том, чтобы правильно измерять прогресс: фиксированные dev‑наборы, FULL‑оценка на фазе 2 и аккуратный верификатор оказались критичными для честного сравнения.
