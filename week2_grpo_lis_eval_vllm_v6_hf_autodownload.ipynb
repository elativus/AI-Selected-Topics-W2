{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec08330",
   "metadata": {},
   "source": [
    "# Week 2 — GRPO LIS: eval (vLLM)\n",
    "\n",
    "Этот ноутбук:\n",
    "- загружает **фиксированные** тестовые наборы (easy/medium/hard) из `data/*.jsonl`\n",
    "- считает accuracy для:\n",
    "  - **baseline** `Qwen/Qwen2.5-1.5B-Instruct`\n",
    "  - **trained** (путь берём из `results/trained_model.json`)\n",
    "- сохраняет числа в `results/baseline_scores.json` и `results/trained_scores.json`\n",
    "- строит **парные бары** (baseline vs trained) как в примере matplotlib.\n",
    "\n",
    "**Важно:** метрики считаются **с тем же `SYSTEM_PROMPT`**, что и в обучении, и инференс делается через **vLLM**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adb98569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если нужно — установи зависимости в eval-окружении\n",
    "# !pip install -U vllm transformers datasets accelerate matplotlib huggingface_hub\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# (опционально) приглушить сетевые warning'и (urllib3)\n",
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95e64750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_DIR       : /home/yaros/DS-Mag/AI-SelectedTopics/W2-1\n",
      "DATA_DIR (initial): /home/yaros/DS-Mag/AI-SelectedTopics/W2-1/data\n",
      "RESULTS_DIR       : /home/yaros/DS-Mag/AI-SelectedTopics/W2-1/results\n",
      "BASE_MODEL        : Qwen/Qwen2.5-1.5B-Instruct\n",
      "TRAINED_MODEL_DIR : /home/yaros/DS-Mag/AI-SelectedTopics/W2-1/results/hf_models/elativus__AI-SelectedTopics-W2\n",
      "HF_MODEL_REPO     : elativus/AI-SelectedTopics-W2\n",
      "HF_DATASET_REPO   : elativus/AI-SelectedTopics-W2\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1) Пути + модели\n",
    "# --------------------------\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_DIR = Path(\".\").resolve()\n",
    "if str(PROJECT_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_DIR))\n",
    "\n",
    "DATA_DIR = PROJECT_DIR / \"data\"\n",
    "RESULTS_DIR = PROJECT_DIR / \"results\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "# SYSTEM_PROMPT должен совпадать с train-ноутбуком (и хранится в w2_utils.py)\n",
    "from w2_utils import SYSTEM_PROMPT\n",
    "\n",
    "#CUSTOM_SYSTEM_PROMPT = SYSTEM_PROMPT\n",
    "CUSTOM_SYSTEM_PROMPT = \"\"\"Отвечай в следующем формате:\n",
    "<think>\n",
    "...\n",
    "</think>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "Output ONLY the integer in <answer> and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "# --------------------------\n",
    "# Hugging Face fallback (опционально)\n",
    "# --------------------------\n",
    "# Если ты запускаешь eval без локальных артефактов train-ноутбука, просто задай:\n",
    "#   export HF_MODEL_REPO=\"username/my-model-repo\"\n",
    "#   export HF_DATASET_REPO=\"username/my-dataset-repo\"\n",
    "# (и при необходимости HF_TOKEN для приватных реп)\n",
    "# Defaults: if local artifacts are missing, we will download from these repos.\n",
    "# You can override them via env vars HF_MODEL_REPO / HF_DATASET_REPO.\n",
    "\n",
    "HF_MODEL_REPO = \"elativus/AI-SelectedTopics-W2\"\n",
    "HF_DATASET_REPO = \"elativus/AI-SelectedTopics-W2\"\n",
    "HF_REVISION = os.getenv(\"HF_REVISION\", \"\").strip() or None\n",
    "\n",
    "HF_TOKEN = (\n",
    "    os.getenv(\"HF_TOKEN\")\n",
    "    or os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "    or os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    ")\n",
    "\n",
    "\n",
    "def _ensure_hf_hub():\n",
    "    \"\"\"Lazy install/import for huggingface_hub.\"\"\"\n",
    "    try:\n",
    "        from huggingface_hub import snapshot_download  # type: ignore\n",
    "        return snapshot_download\n",
    "    except Exception:\n",
    "        import subprocess, sys\n",
    "        print(\"[INFO] Installing huggingface_hub ...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-qU\", \"huggingface_hub\"])\n",
    "        from huggingface_hub import snapshot_download  # type: ignore\n",
    "        return snapshot_download\n",
    "\n",
    "\n",
    "def _sanitize_repo_id(repo_id: str) -> str:\n",
    "    return repo_id.replace(\"/\", \"__\")\n",
    "\n",
    "\n",
    "def _snapshot_download(repo_id: str, repo_type: str, local_dir: Path) -> Path:\n",
    "    snapshot_download = _ensure_hf_hub()\n",
    "    local_dir = Path(local_dir)\n",
    "    local_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"[HF] snapshot_download(repo_id='{repo_id}', repo_type='{repo_type}') -> {local_dir}\")\n",
    "\n",
    "    # huggingface_hub менял API: token vs use_auth_token — поддержим оба варианта\n",
    "    kwargs = dict(\n",
    "        repo_id=repo_id,\n",
    "        repo_type=repo_type,\n",
    "        local_dir=str(local_dir),\n",
    "        local_dir_use_symlinks=False,\n",
    "    )\n",
    "    if HF_REVISION:\n",
    "        kwargs[\"revision\"] = HF_REVISION\n",
    "    if HF_TOKEN:\n",
    "        kwargs[\"token\"] = HF_TOKEN\n",
    "\n",
    "    try:\n",
    "        snapshot_download(**kwargs)\n",
    "    except TypeError:\n",
    "        # older hub versions\n",
    "        if \"token\" in kwargs:\n",
    "            kwargs[\"use_auth_token\"] = kwargs.pop(\"token\")\n",
    "        # local_dir_use_symlinks may also be absent\n",
    "        try:\n",
    "            snapshot_download(**kwargs)\n",
    "        except TypeError:\n",
    "            kwargs.pop(\"local_dir_use_symlinks\", None)\n",
    "            snapshot_download(**kwargs)\n",
    "\n",
    "    return local_dir\n",
    "\n",
    "\n",
    "def _looks_like_model_dir(p: Path) -> bool:\n",
    "    if not p.exists() or not p.is_dir():\n",
    "        return False\n",
    "    has_cfg = (p / \"config.json\").exists()\n",
    "    has_weights = bool(list(p.glob(\"*.safetensors\"))) or (p / \"pytorch_model.bin\").exists() or (p / \"model.safetensors\").exists()\n",
    "    return bool(has_cfg and has_weights)\n",
    "\n",
    "\n",
    "# trained_model.json создаётся train-ноутбуком: {\"trained_model_dir\": \"...\"}\n",
    "trained_model_json = RESULTS_DIR / \"trained_model.json\"\n",
    "\n",
    "TRAINED_MODEL_DIR = None\n",
    "\n",
    "# 1) primary: results/trained_model.json\n",
    "if trained_model_json.exists():\n",
    "    try:\n",
    "        cand_raw = json.loads(trained_model_json.read_text(encoding=\"utf-8\"))[\"trained_model_dir\"]\n",
    "        cand = Path(cand_raw)\n",
    "        if not cand.is_absolute():\n",
    "            cand = (PROJECT_DIR / cand).resolve()\n",
    "        if _looks_like_model_dir(cand):\n",
    "            TRAINED_MODEL_DIR = str(cand)\n",
    "        else:\n",
    "            print(f\"[WARN] {trained_model_json} найден, но папка модели не найдена/непохожа на модель: {cand}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Не удалось прочитать {trained_model_json}: {e}\")\n",
    "\n",
    "# 2) fallback: стандартный путь, который использует train-ноутбук\n",
    "if TRAINED_MODEL_DIR is None:\n",
    "    default_local = PROJECT_DIR / \"models\" / \"qwen2p5_1p5b_grpo_lis_merged\"\n",
    "    if _looks_like_model_dir(default_local):\n",
    "        TRAINED_MODEL_DIR = str(default_local)\n",
    "\n",
    "# 3) fallback: скачать с HF, если задан HF_MODEL_REPO\n",
    "if TRAINED_MODEL_DIR is None:\n",
    "    if HF_MODEL_REPO:\n",
    "        target = RESULTS_DIR / \"hf_models\" / _sanitize_repo_id(HF_MODEL_REPO)\n",
    "        target = _snapshot_download(HF_MODEL_REPO, repo_type=\"model\", local_dir=target)\n",
    "        TRAINED_MODEL_DIR = str(target)\n",
    "\n",
    "        # сохранить указатель, чтобы другие части репы (и этот ноутбук) видели путь\n",
    "        trained_model_json.write_text(\n",
    "            json.dumps({\"trained_model_dir\": TRAINED_MODEL_DIR}, ensure_ascii=False, indent=2),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Не найден {trained_model_json} и локальная папка merged-модели.\\n\"\n",
    "            f\"Варианты:\\n\"\n",
    "            f\"  1) Запусти train-ноутбук (он создаст results/trained_model.json и models/*)\\n\"\n",
    "            f\"  2) Или задай переменную окружения HF_MODEL_REPO (например: export HF_MODEL_REPO='username/my-model')\\n\"\n",
    "        )\n",
    "\n",
    "print(\"PROJECT_DIR       :\", PROJECT_DIR)\n",
    "print(\"DATA_DIR (initial):\", DATA_DIR)\n",
    "print(\"RESULTS_DIR       :\", RESULTS_DIR)\n",
    "print(\"BASE_MODEL        :\", BASE_MODEL)\n",
    "print(\"TRAINED_MODEL_DIR :\", TRAINED_MODEL_DIR)\n",
    "print(\"HF_MODEL_REPO     :\", HF_MODEL_REPO)\n",
    "print(\"HF_DATASET_REPO   :\", HF_DATASET_REPO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bba7697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HF] snapshot_download(repo_id='elativus/AI-SelectedTopics-W2', repo_type='dataset') -> /home/yaros/DS-Mag/AI-SelectedTopics/W2-1/hf_datasets/elativus__AI-SelectedTopics-W2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaros/DS-Mag/AI-SelectedTopics/W2-1/.eval-env/lib/python3.12/site-packages/huggingface_hub/file_download.py:986: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec12eb8eef84085837af501c44352bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR (resolved): /home/yaros/DS-Mag/AI-SelectedTopics/W2-1/hf_datasets/elativus__AI-SelectedTopics-W2\n",
      "[INFO] Loaded test_specs from dataset_index.json: 3\n",
      "Loaded testsets: {'easy': 200, 'hard': 200, 'medium': 200}\n",
      "eval_max_new_tokens = 64\n",
      "eval_temperature    = 0.0\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 2) Загрузка фиксированных testsets (jsonl)\n",
    "#    В train-ноутбуке строки сохранялись как {\"prompt\": ..., \"answer\": ...}\n",
    "# --------------------------\n",
    "from w2_utils import load_jsonl, get_user_prompt\n",
    "\n",
    "# Если локально нет test_*.jsonl — попробуем скачать датасеты с HF (если задан HF_DATASET_REPO)\n",
    "def _has_any_test_jsonl(root: Path) -> bool:\n",
    "    return bool(list(root.glob(\"test_*.jsonl\")) or list((root / \"test\").glob(\"test_*.jsonl\")))\n",
    "\n",
    "if not _has_any_test_jsonl(DATA_DIR):\n",
    "    if HF_DATASET_REPO:\n",
    "        hf_ds_root = PROJECT_DIR / \"hf_datasets\" / _sanitize_repo_id(HF_DATASET_REPO)\n",
    "        DATA_DIR = _snapshot_download(HF_DATASET_REPO, repo_type=\"dataset\", local_dir=hf_ds_root)\n",
    "    else:\n",
    "        print(\n",
    "            \"[WARN] В DATA_DIR нет test_*.jsonl и HF_DATASET_REPO не задан. \"\n",
    "            \"Буду искать локально по ожидаемым именам и упаду с ошибкой, если не найду.\"\n",
    "        )\n",
    "\n",
    "print(\"DATA_DIR (resolved):\", DATA_DIR)\n",
    "\n",
    "# Подхватываем спецификацию тестов из results/config.json, если она есть\n",
    "config_json = RESULTS_DIR / \"config.json\"\n",
    "if config_json.exists():\n",
    "    cfg = json.loads(config_json.read_text(encoding=\"utf-8\"))\n",
    "    test_specs = cfg.get(\"test_specs\", [])\n",
    "    eval_max_new_tokens = int(cfg.get(\"eval_max_new_tokens\", 64))\n",
    "    eval_temperature = float(cfg.get(\"eval_temperature\", 0.0))\n",
    "else:\n",
    "    cfg = {}\n",
    "    test_specs = []\n",
    "    eval_max_new_tokens = 64\n",
    "    eval_temperature = 0.0\n",
    "\n",
    "# Если config.json не найден / без test_specs, пробуем взять список файлов из dataset_index.json (если он есть в HF-датасете)\n",
    "ds_index_path = DATA_DIR / \"dataset_index.json\"\n",
    "if (not test_specs) and ds_index_path.exists():\n",
    "    try:\n",
    "        ds_index = json.loads(ds_index_path.read_text(encoding=\"utf-8\"))\n",
    "        files_test = (ds_index.get(\"files\") or {}).get(\"test\") or {}\n",
    "        test_specs = []\n",
    "        for name, entry in files_test.items():\n",
    "            # entry: {\"filename\": ..., \"difficulty\": ..., \"n\": ..., \"seed\": ...}\n",
    "            test_specs.append({\n",
    "                \"name\": name,\n",
    "                \"filename\": entry.get(\"filename\") or entry.get(\"path_in_repo\", \"\").split(\"/\")[-1],\n",
    "                \"difficulty\": entry.get(\"difficulty\"),\n",
    "                \"n\": entry.get(\"n\"),\n",
    "                \"seed\": entry.get(\"seed\"),\n",
    "            })\n",
    "        print(f\"[INFO] Loaded test_specs from dataset_index.json: {len(test_specs)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to parse dataset_index.json: {e}\")\n",
    "\n",
    "# fallback если и dataset_index.json нет, используем дефолтный список\n",
    "if not test_specs:\n",
    "    test_specs = [\n",
    "        {\"name\": \"easy\",   \"difficulty\": 2, \"n\": 200, \"seed\": 1001},\n",
    "        {\"name\": \"medium\", \"difficulty\": 5, \"n\": 200, \"seed\": 2001},\n",
    "        {\"name\": \"hard\",   \"difficulty\": 8, \"n\": 200, \"seed\": 3001},\n",
    "    ]\n",
    "\n",
    "def _resolve_jsonl(kind: str, filename: str) -> Path:\n",
    "    \"\"\"Ищем файл как в локальном data/, так и в структуре HF-датасета (kind/filename).\"\"\"\n",
    "    candidates = [\n",
    "        DATA_DIR / filename,\n",
    "        DATA_DIR / kind / filename,\n",
    "        (PROJECT_DIR / \"data\") / filename,\n",
    "        (PROJECT_DIR / \"data\") / kind / filename,\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            return c\n",
    "    # helpful error\n",
    "    cand_str = \"\\n\".join([f\"  - {c}\" for c in candidates])\n",
    "    raise FileNotFoundError(\n",
    "        f\"Не найден датасет-файл '{filename}'. Пробовал:\\n{cand_str}\\n\\n\"\n",
    "        f\"Если ты хочешь авто-скачивание с HF, задай HF_DATASET_REPO='username/my-dataset'.\"\n",
    "    )\n",
    "\n",
    "testsets = {}\n",
    "for spec in test_specs:\n",
    "    name = spec[\"name\"]\n",
    "\n",
    "    # filename может быть явно задан (через dataset_index.json), иначе собираем по шаблону\n",
    "    if spec.get(\"filename\"):\n",
    "        filename = spec[\"filename\"]\n",
    "    else:\n",
    "        filename = f\"test_{name}_d{spec['difficulty']}_n{spec['n']}_seed{spec['seed']}.jsonl\"\n",
    "\n",
    "    p = _resolve_jsonl(\"test\", filename)\n",
    "    rows = load_jsonl(p)\n",
    "\n",
    "    # sanity check\n",
    "    _ = get_user_prompt(rows[0])\n",
    "    _ = rows[0][\"answer\"]\n",
    "\n",
    "    testsets[name] = Dataset.from_list(rows)\n",
    "\n",
    "print(\"Loaded testsets:\", {k: len(v) for k, v in testsets.items()})\n",
    "print(\"eval_max_new_tokens =\", eval_max_new_tokens)\n",
    "print(\"eval_temperature    =\", eval_temperature)\n",
    "\n",
    "\n",
    "CUSTOM_SYSTEM_PROMPT = \"\"\"Respond in the following format::\n",
    "<think>\n",
    "...\n",
    "</think>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "Output ONLY the integer in <answer> and nothing else.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a692aa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 3) Prompt builder + extractor\n",
    "# --------------------------\n",
    "from w2_utils import build_chat_prompt, extract_int\n",
    "\n",
    "# Для отладочной статистики (есть ли <answer> тег в ответе)\n",
    "ANSWER_RE = re.compile(r\"<answer>.*<\\/answer>\", re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b822247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 4) vLLM: один LLM на модель + оценка на всех датасетах\n",
    "# --------------------------\n",
    "from w2_utils import eval_all_testsets_with_llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2fd8b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-28 19:31:02 [utils.py:223] non-default args: {'tokenizer': 'Qwen/Qwen2.5-1.5B-Instruct', 'trust_remote_code': True, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'Qwen/Qwen2.5-1.5B-Instruct'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-28 19:31:03 [model.py:529] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 02-28 19:31:03 [model.py:1549] Using max model len 32768\n",
      "INFO 02-28 19:31:03 [scheduler.py:224] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 02-28 19:31:03 [vllm.py:727] Enforce eager set, overriding optimization level to -O0\n",
      "INFO 02-28 19:31:03 [vllm.py:845] Cudagraph is disabled under eager mode\n",
      "(EngineCore_DP0 pid=30873) INFO 02-28 19:31:07 [core.py:97] Initializing a V1 LLM engine (v0.16.0) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [16384], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False, 'fuse_act_padding': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': False}, 'local_cache_dir': None, 'fast_moe_cold_start': True, 'static_all_moe_layers': []}\n",
      "(EngineCore_DP0 pid=30873) INFO 02-28 19:31:08 [parallel_state.py:1234] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.22.107.93:36119 backend=nccl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W228 19:31:18.925040369 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(EngineCore_DP0 pid=30873) INFO 02-28 19:31:18 [parallel_state.py:1445] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A\n",
      "(EngineCore_DP0 pid=30873) WARNING 02-28 19:31:18 [interface.py:456] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "(EngineCore_DP0 pid=30873) INFO 02-28 19:31:18 [gpu_model_runner.py:4124] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(EngineCore_DP0 pid=30873) /home/yaros/DS-Mag/AI-SelectedTopics/W2-1/.eval-env/lib/python3.12/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:181: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "(EngineCore_DP0 pid=30873) We recommend installing via `pip install torch-c-dlpack-ext`\n",
      "(EngineCore_DP0 pid=30873)   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(EngineCore_DP0 pid=30873) INFO 02-28 19:31:19 [cuda.py:367] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION'].\n",
      "(EngineCore_DP0 pid=30873) INFO 02-28 19:31:20 [weight_utils.py:579] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.58it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.58it/s]\n",
      "(EngineCore_DP0 pid=30873) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(EngineCore_DP0 pid=30873) INFO 02-28 19:31:20 [default_loader.py:293] Loading weights took 0.36 seconds\n",
      "(EngineCore_DP0 pid=30873) INFO 02-28 19:31:21 [gpu_model_runner.py:4221] Model loading took 2.89 GiB memory and 1.725850 seconds\n",
      "(EngineCore_DP0 pid=30873) INFO 02-28 19:31:22 [gpu_worker.py:373] Available KV cache memory: 20.2 GiB\n",
      "(EngineCore_DP0 pid=30873) INFO 02-28 19:31:22 [kv_cache_utils.py:1307] GPU KV cache size: 756,304 tokens\n",
      "(EngineCore_DP0 pid=30873) INFO 02-28 19:31:22 [kv_cache_utils.py:1312] Maximum concurrency for 32,768 tokens per request: 23.08x\n",
      "(EngineCore_DP0 pid=30873) INFO 02-28 19:31:24 [kernel_warmup.py:44] Skipping FlashInfer autotune because it is disabled.\n",
      "(EngineCore_DP0 pid=30873) INFO 02-28 19:31:25 [core.py:278] init engine (profile, create kv cache, warmup model) took 4.37 seconds\n",
      "(EngineCore_DP0 pid=30873) INFO 02-28 19:31:26 [vllm.py:689] Asynchronous scheduling is enabled.\n",
      "(EngineCore_DP0 pid=30873) WARNING 02-28 19:31:26 [vllm.py:734] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.\n",
      "(EngineCore_DP0 pid=30873) INFO 02-28 19:31:26 [vllm.py:845] Cudagraph is disabled under eager mode\n",
      "INFO 02-28 19:31:27 [llm.py:355] Supported tasks: ['generate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W228 19:31:27.761037584 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BASELINE ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25798dfd31f145fbb2db633fe2bf7054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2df6c33f47b4ef9bbdebe5ae7e0bfe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[easy] acc=0.0450 (n=200)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99550fbdaef745be86bc58473c62632f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168d74f3ca054ad38d1b4cd769cfee49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hard] acc=0.0350 (n=200)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b3f86703414642ace910ee44c03cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c1c409bc8a4f74993b6d91e0950151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[medium] acc=0.1100 (n=200)\n",
      "INFO 02-28 19:31:28 [utils.py:223] non-default args: {'tokenizer': 'Qwen/Qwen2.5-1.5B-Instruct', 'trust_remote_code': True, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'enforce_eager': True, 'model': '/home/yaros/DS-Mag/AI-SelectedTopics/W2-1/results/hf_models/elativus__AI-SelectedTopics-W2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-28 19:31:28 [model.py:529] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 02-28 19:31:28 [model.py:1549] Using max model len 32768\n",
      "INFO 02-28 19:31:28 [scheduler.py:224] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 02-28 19:31:28 [vllm.py:727] Enforce eager set, overriding optimization level to -O0\n",
      "INFO 02-28 19:31:28 [vllm.py:845] Cudagraph is disabled under eager mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(EngineCore_DP0 pid=31181) INFO 02-28 19:31:32 [core.py:97] Initializing a V1 LLM engine (v0.16.0) with config: model='/home/yaros/DS-Mag/AI-SelectedTopics/W2-1/results/hf_models/elativus__AI-SelectedTopics-W2', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/yaros/DS-Mag/AI-SelectedTopics/W2-1/results/hf_models/elativus__AI-SelectedTopics-W2, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [16384], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False, 'fuse_act_padding': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': False}, 'local_cache_dir': None, 'fast_moe_cold_start': True, 'static_all_moe_layers': []}\n",
      "(EngineCore_DP0 pid=31181) INFO 02-28 19:31:33 [parallel_state.py:1234] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.22.107.93:49307 backend=nccl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W228 19:31:43.909172885 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(EngineCore_DP0 pid=31181) INFO 02-28 19:31:43 [parallel_state.py:1445] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A\n",
      "(EngineCore_DP0 pid=31181) WARNING 02-28 19:31:43 [interface.py:456] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "(EngineCore_DP0 pid=31181) INFO 02-28 19:31:43 [gpu_model_runner.py:4124] Starting to load model /home/yaros/DS-Mag/AI-SelectedTopics/W2-1/results/hf_models/elativus__AI-SelectedTopics-W2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(EngineCore_DP0 pid=31181) /home/yaros/DS-Mag/AI-SelectedTopics/W2-1/.eval-env/lib/python3.12/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:181: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "(EngineCore_DP0 pid=31181) We recommend installing via `pip install torch-c-dlpack-ext`\n",
      "(EngineCore_DP0 pid=31181)   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(EngineCore_DP0 pid=31181) INFO 02-28 19:31:44 [cuda.py:367] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION'].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.30it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.30it/s]\n",
      "(EngineCore_DP0 pid=31181) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(EngineCore_DP0 pid=31181) INFO 02-28 19:31:44 [default_loader.py:293] Loading weights took 0.39 seconds\n",
      "(EngineCore_DP0 pid=31181) INFO 02-28 19:31:45 [gpu_model_runner.py:4221] Model loading took 2.89 GiB memory and 0.721021 seconds\n",
      "(EngineCore_DP0 pid=31181) INFO 02-28 19:31:46 [gpu_worker.py:373] Available KV cache memory: 20.2 GiB\n",
      "(EngineCore_DP0 pid=31181) INFO 02-28 19:31:46 [kv_cache_utils.py:1307] GPU KV cache size: 756,304 tokens\n",
      "(EngineCore_DP0 pid=31181) INFO 02-28 19:31:46 [kv_cache_utils.py:1312] Maximum concurrency for 32,768 tokens per request: 23.08x\n",
      "(EngineCore_DP0 pid=31181) INFO 02-28 19:31:46 [kernel_warmup.py:44] Skipping FlashInfer autotune because it is disabled.\n",
      "(EngineCore_DP0 pid=31181) INFO 02-28 19:31:46 [core.py:278] init engine (profile, create kv cache, warmup model) took 1.54 seconds\n",
      "(EngineCore_DP0 pid=31181) INFO 02-28 19:31:48 [vllm.py:689] Asynchronous scheduling is enabled.\n",
      "(EngineCore_DP0 pid=31181) WARNING 02-28 19:31:48 [vllm.py:734] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.\n",
      "(EngineCore_DP0 pid=31181) INFO 02-28 19:31:48 [vllm.py:845] Cudagraph is disabled under eager mode\n",
      "INFO 02-28 19:31:48 [llm.py:355] Supported tasks: ['generate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W228 19:31:48.292542284 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAINED ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23535c49e3314f5082dbc2662ed16d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70f619aca3e475fba2b08eba16067e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[easy] acc=0.2100 (n=200)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf377b24bbf4116b7fd1f9b559bdba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aae91e6ba59483ebbe267baf3b02167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hard] acc=0.3150 (n=200)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed837fb80dad42368fa819b60c4bb438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea08a611e5e44ac8bae35d935b306e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[medium] acc=0.3150 (n=200)\n",
      "\n",
      "Saved:\n",
      " - /home/yaros/DS-Mag/AI-SelectedTopics/W2-1/results/baseline_scores.json\n",
      " - /home/yaros/DS-Mag/AI-SelectedTopics/W2-1/results/trained_scores.json\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 5) Запуск eval: baseline vs trained\n",
    "# --------------------------\n",
    "# tokenizer используем от BASE_MODEL (после RL он обычно не меняется)\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "gpu_mem_util = 0.3\n",
    "# baseline LLM\n",
    "baseline_llm = LLM(\n",
    "    model=BASE_MODEL,\n",
    "    tokenizer=BASE_MODEL,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"auto\",\n",
    "    enforce_eager=True,\n",
    "    gpu_memory_utilization=gpu_mem_util\n",
    ")\n",
    "\n",
    "print(\"\\n=== BASELINE ===\")\n",
    "baseline_scores = eval_all_testsets_with_llm(\n",
    "    baseline_llm, tok, testsets,\n",
    "    system_prompt=CUSTOM_SYSTEM_PROMPT,\n",
    "    max_new_tokens=eval_max_new_tokens,\n",
    "    temperature=eval_temperature,\n",
    ")\n",
    "\n",
    "# Удаляем baseline движок перед созданием trained\n",
    "# del baseline_llm\n",
    "# import gc, torch, time\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# time.sleep(2)\n",
    "\n",
    "# trained LLM (merged dir)\n",
    "trained_llm = LLM(\n",
    "    model=TRAINED_MODEL_DIR,\n",
    "    tokenizer=BASE_MODEL,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"auto\",\n",
    "    enforce_eager=True,\n",
    "    gpu_memory_utilization=gpu_mem_util\n",
    ")\n",
    "\n",
    "print(\"\\n=== TRAINED ===\")\n",
    "trained_scores = eval_all_testsets_with_llm(\n",
    "    trained_llm, tok, testsets,\n",
    "    system_prompt=CUSTOM_SYSTEM_PROMPT,\n",
    "    max_new_tokens=eval_max_new_tokens,\n",
    "    temperature=eval_temperature,\n",
    ")\n",
    "\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(RESULTS_DIR / \"baseline_scores.json\").write_text(json.dumps(baseline_scores, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "(RESULTS_DIR / \"trained_scores.json\").write_text(json.dumps(trained_scores, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" -\", RESULTS_DIR / \"baseline_scores.json\")\n",
    "print(\" -\", RESULTS_DIR / \"trained_scores.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "123879ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary ===\n",
      "    easy: baseline=0.0450 | trained=0.2100 | delta=+0.1650\n",
      "    hard: baseline=0.0350 | trained=0.3150 | delta=+0.2800\n",
      "  medium: baseline=0.1100 | trained=0.3150 | delta=+0.2050\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 6) Числа + дельты (trained - baseline)\n",
    "# --------------------------\n",
    "names = list(testsets.keys())\n",
    "print(\"\\n=== Summary ===\")\n",
    "for n in names:\n",
    "    b = baseline_scores[n]\n",
    "    t = trained_scores[n]\n",
    "    print(f\"{n:>8}: baseline={b:.4f} | trained={t:.4f} | delta={t-b:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testset sizes: {'easy': 200, 'hard': 200, 'medium': 200}\n",
      "\n",
      "=== easy sample ===\n",
      "PROMPT (head): You are given a sequence of integers.  Task: compute the length of the Longest Increasing Subsequence (LIS).  Definitions: - A subsequence is obtained by deleting zero or more elements without changin\n",
      "GOLD: 4\n",
      "\n",
      "=== hard sample ===\n",
      "PROMPT (head): You are given a sequence of integers.  Task: compute the length of the Longest Increasing Subsequence (LIS).  Definitions: - A subsequence is obtained by deleting zero or more elements without changin\n",
      "GOLD: 7\n",
      "\n",
      "=== medium sample ===\n",
      "PROMPT (head): You are given a sequence of integers.  Task: compute the length of the Longest Increasing Subsequence (LIS).  Definitions: - A subsequence is obtained by deleting zero or more elements without changin\n",
      "GOLD: 5\n",
      "\n",
      "===== DEBUG baseline =====\n",
      "\n",
      "--- dataset easy (k=3) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e1c1df3f9140e69215e8fa035a755d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d224c5b27343429731f40e236cfde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=4 pred=4 ok=True\n",
      "out_tail: ll maintain an array `dp` where `dp[i]` represents the length of the LIS ending at index `i`. We'll iterate through the sequence and update the `dp` array accordingly. The length of the LIS will be the maximum value in the `dp` array. </think>  <answer> - The length of the Longest Increasing Subsequence is 4. </answer>\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc54a1db0974e5e9ad0ab62576cde61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5d79a11d01416ca4f4331b4b1b41f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=3 pred=4 ok=False\n",
      "out_tail: ray `dp` where `dp[i]` represents the length of the LIS ending with the element at index `i`. We'll iterate through the sequence and update the `dp` array accordingly. The length of the LIS will be the maximum value in the `dp` array. </think>  <answer> - The length of the Longest Increasing Subsequence is 4. </answer>\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ad83cd979e4115a6f90a97a6ecd631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c96dd21eeb4ce9966a810970c4c4a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=4 pred=10 ok=False\n",
      "out_tail:  ending at index `i`. We iterate through the sequence and update the `dp` array accordingly. The maximum value in the `dp` array will give us the length of the LIS. </think>  <answer> [1, 2, 2, 2, 1, 2, 2, 3, 1, 1] </answer>  The LIS of the given sequence is [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]. The length of this LIS is 10.\n",
      "answer_tag: None\n",
      "subset_acc=0.333 | parsed_rate=1.000 | has_answer_tag_rate=0.000\n",
      "\n",
      "--- dataset hard (k=3) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f16e6c3e164298929f54c0966ad98e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab97973c97d479f9a0da8cac6a9f0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=7 pred=None ok=False\n",
      "out_tail: p` where `dp[i]` represents the length of the LIS ending with the element at index `i`. We will iterate through the sequence and update the `dp` array accordingly. The length of the LIS will be the maximum value in the `dp` array. </think>  <answer> [2, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5] </answer>\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e591b1d67c2442f697f84984afc24e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f68bc4ab1b42a4beeb8b818767c2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=7 pred=None ok=False\n",
      "out_tail: re `dp[i]` represents the length of the LIS ending with the element at index `i`. We will iterate through the sequence and update the `dp` array accordingly. The length of the LIS will be the maximum value in the `dp` array. </think>  <answer> [1, 1, 2, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7] </answer>\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1ba9ce3c06414e9534c3a26a508512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b9b86704d24db5a0f6929e62aba9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=7 pred=None ok=False\n",
      "out_tail: he length of the LIS ending with the element at index `i`. We will iterate through the sequence and update the `dp` array accordingly. The length of the LIS will be the maximum value in the `dp` array. </think>  <answer> -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 </answer>  The length of the LIS is 11.\n",
      "answer_tag: None\n",
      "subset_acc=0.000 | parsed_rate=0.000 | has_answer_tag_rate=0.000\n",
      "\n",
      "--- dataset medium (k=3) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186b666a35de42abad04129222612c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f52905bd47b4625b6f12872e1692121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=5 pred=-13 ok=False\n",
      "out_tail: can use dynamic programming. We will maintain an array `dp` where `dp[i]` represents the length of the LIS ending with the element at index `i`. We will iterate through the sequence and update the `dp` array accordingly. The length of the LIS will be the maximum value in the `dp` array. </think>  <answer> -13 </answer>\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775e660ce07b4f82a063292485c8a13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1322a69fb8c415e8e44abd6add2a6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=6 pred=2 ok=False\n",
      "out_tail: ray `dp` where `dp[i]` represents the length of the LIS ending at index `i`. We iterate through the sequence and update the `dp` array accordingly. The maximum value in the `dp` array will give us the length of the LIS. </think>  <answer> -20, 20, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2 </answer>  The LIS length is 2.\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b74265b13f54ee88c0921bae33f422b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061f70fbc3404f43ba8cf9fa8e209ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=6 pred=-11 ok=False\n",
      "out_tail: quence (LIS), we can use dynamic programming. We will maintain an array `dp` where `dp[i]` represents the length of the LIS ending at index `i`. We iterate through the sequence and update the `dp` array accordingly. The maximum value in the `dp` array will give us the length of the LIS. </think>  <answer> -11 </answer>\n",
      "answer_tag: None\n",
      "subset_acc=0.000 | parsed_rate=1.000 | has_answer_tag_rate=0.000\n",
      "\n",
      "===== DEBUG trained =====\n",
      "\n",
      "--- dataset easy (k=3) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16b3198c42f40f28d9a7fd8150cc903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45bf6feccdc4614bfc3672c648cde43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=4 pred=3 ok=False\n",
      "out_tail: <answer> 3 </answer> <think> To find the length of the Longest Increasing Subsequence (LIS), we need to identify the longest subsequence of the given sequence where each element is greater than the previous one. We can use dynamic programming to solve this problem efficiently. </think>\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "048f83c92b3b44d2bc264a5131df6cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8206f4b867411fb884a6fddf4bf4a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=3 pred=4 ok=False\n",
      "out_tail: <answer> 4 </answer> <think> To find the length of the Longest Increasing Subsequence (LIS), we need to identify the longest subsequence of the given sequence where each element is strictly greater than the previous one. We can use dynamic programming to solve this problem efficiently. </think>\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b59c0a49874bb3a1f8d8d40019456e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707f9daea1a6491fa27c575d84262bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=4 pred=4 ok=True\n",
      "out_tail: <answer> 4 </answer> <think> To find the length of the Longest Increasing Subsequence (LIS), we need to identify the longest subsequence of the given sequence where each element is greater than the previous one. We can use dynamic programming to solve this problem efficiently. </think>\n",
      "answer_tag: None\n",
      "subset_acc=0.333 | parsed_rate=1.000 | has_answer_tag_rate=0.000\n",
      "\n",
      "--- dataset hard (k=3) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad27259cdd0400587da39b834d36e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14190c03fb21489681d45bd707c58f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=7 pred=4 ok=False\n",
      "out_tail: <answer> 4 </answer> <think> To find the length of the Longest Increasing Subsequence (LIS), we need to identify the longest subsequence of the given sequence where each element is strictly greater than the previous one. We can use dynamic programming to solve this problem efficiently. </think>\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e397bbed590474f81a909fdaefea1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d957932b395849f38e53aff416c6c92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=7 pred=4 ok=False\n",
      "out_tail: uence (LIS), we can use dynamic programming. We will maintain an array `dp` where `dp[i]` represents the length of the LIS ending at index `i`. We iterate through the sequence and for each element, we check all previous elements to see if they can be part of the LIS. If they can, we update `dp[i]` accordingly. </think>\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462e50abbd544aaaaa7ccc420f2c565d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153b4c1bca3d45738d52a59bb4aaf03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=7 pred=5 ok=False\n",
      "out_tail: <answer> 5 </answer>\n",
      "answer_tag: None\n",
      "subset_acc=0.000 | parsed_rate=1.000 | has_answer_tag_rate=0.000\n",
      "\n",
      "--- dataset medium (k=3) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56d620d10c944fb8743605ae560de47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1130dc908d4347b49aa4d3ba202457ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=5 pred=4 ok=False\n",
      "out_tail: <answer> 4 </answer>\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046ef361fedf4fcb9794562051cc7104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8677f51b05946c2b78c36285f300e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=6 pred=4 ok=False\n",
      "out_tail: <answer> 4 </answer>\n",
      "answer_tag: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612e72d8e6c84841b8b556b9d352a8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f99651cac7b4136bfe4a121e3f95f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold=6 pred=4 ok=False\n",
      "out_tail: <answer> 4 </answer>\n",
      "answer_tag: None\n",
      "subset_acc=0.000 | parsed_rate=1.000 | has_answer_tag_rate=0.000\n",
      "\n",
      "=== TRAINED on saved dev_* (sanity) ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171cf29df7f64cd28e4f6a336c613477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdda7428fb444e93a6a2aa24dbd7570a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dev_easy_d2_n128_seed4001] acc=0.1719 (n=128)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465c33cfb5e74c5d95f49e2a7ed9c157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7d9845a4a74051b7121fe1ba4a9604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dev_hard_d8_n128_seed4003] acc=0.3438 (n=128)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c4474388a342c4bbf1f81624805211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0754551baf4eb6936cc3aa7af8b677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dev_medium_d5_n128_seed4002] acc=0.2812 (n=128)\n",
      "dev_easy_d2_n128_seed4001: acc=0.1719 (n=128)\n",
      "dev_hard_d8_n128_seed4003: acc=0.3438 (n=128)\n",
      "dev_medium_d5_n128_seed4002: acc=0.2812 (n=128)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 7) DEBUG / sanity-check generation & parsing\n",
    "# --------------------------\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "DEBUG_K = 3\n",
    "DEBUG_MAX_NEW_TOKENS = 256\n",
    "\n",
    "def _preview_examples(ds, n: int = 1) -> None:\n",
    "    n = int(n)\n",
    "    for i in range(min(n, len(ds))):\n",
    "        ex = ds[i]\n",
    "        p = get_user_prompt(ex)\n",
    "        print(\"PROMPT (head):\", p[:200].replace(\"\\n\", \" \"))\n",
    "        print(\"GOLD:\", ex.get(\"answer\"))\n",
    "\n",
    "print(\"Testset sizes:\", {k: len(v) for k, v in testsets.items()})\n",
    "for name, ds in testsets.items():\n",
    "    print(f\"\\n=== {name} sample ===\")\n",
    "    _preview_examples(ds, n=1)\n",
    "\n",
    "def debug_generate_one(llm, ex: Dict[str, Any], max_new_tokens: int = 128) -> Tuple[Optional[int], str]:\n",
    "    user_prompt = get_user_prompt(ex)\n",
    "    prompt_text = build_chat_prompt(tok, user_prompt)\n",
    "\n",
    "    params = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        max_tokens=int(max_new_tokens),\n",
    "        #stop=[\"</answer>\"],  # same as eval (can remove stop if you want)\n",
    "    )\n",
    "    out_text = llm.generate([prompt_text], params)[0].outputs[0].text\n",
    "    pred = extract_int(out_text)\n",
    "    with open(\"log.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(out_text + \"\\n\")\n",
    "        f.write(\"pred=\"+ str(pred) + \"\\n\")\n",
    "    return pred, out_text\n",
    "\n",
    "def run_debug_on_subset(llm, datasets: Dict[str, Any], model_name: str) -> None:\n",
    "    print(f\"\\n===== DEBUG {model_name} =====\")\n",
    "    for ds_name, ds in datasets.items():\n",
    "        k = min(int(DEBUG_K), len(ds))\n",
    "        if k <= 0:\n",
    "            print(f\"--- dataset {ds_name}: empty ---\")\n",
    "            continue\n",
    "\n",
    "        correct = 0\n",
    "        parsed = 0\n",
    "        has_tag = 0\n",
    "\n",
    "        print(f\"\\n--- dataset {ds_name} (k={k}) ---\")\n",
    "        for i in range(k):\n",
    "            ex = ds[i]  # IMPORTANT: Dataset[:k] returns dict-of-columns -> BUG. Use index!\n",
    "\n",
    "            pred, out_text = debug_generate_one(llm, ex, max_new_tokens=DEBUG_MAX_NEW_TOKENS)\n",
    "\n",
    "            gold_raw = ex.get(\"answer\", None)\n",
    "            try:\n",
    "                gold = int(gold_raw) if gold_raw is not None else None\n",
    "            except Exception:\n",
    "                gold = None\n",
    "\n",
    "            ok = (pred is not None and gold is not None and int(pred) == int(gold))\n",
    "            correct += int(ok)\n",
    "            parsed  += int(pred is not None)\n",
    "            has_tag += int(ANSWER_RE.search(out_text) is not None)\n",
    "\n",
    "            print(f\"gold={gold} pred={pred} ok={ok}\")\n",
    "            print(\"out_tail:\", out_text[-320:].replace(\"\\n\",\" \"))\n",
    "\n",
    "            m = ANSWER_RE.search(out_text)\n",
    "            print(\"answer_tag:\", m.group(0) if m else None)\n",
    "\n",
    "        print(f\"subset_acc={correct/k:.3f} | parsed_rate={parsed/k:.3f} | has_answer_tag_rate={has_tag/k:.3f}\")\n",
    "\n",
    "# Small sanity-check on real tasks (prints a few generations)\n",
    "run_debug_on_subset(baseline_llm, testsets, \"baseline\")\n",
    "run_debug_on_subset(trained_llm,  testsets, \"trained\")\n",
    "\n",
    "# --------------------------\n",
    "# Extra sanity: evaluate TRAINED on saved dev_*.jsonl from train-notebook (if present)\n",
    "# --------------------------\n",
    "dev_files = sorted(DATA_DIR.glob(\"dev_*.jsonl\"))\n",
    "if not dev_files:\n",
    "    dev_files = sorted((DATA_DIR / \"dev\").glob(\"dev_*.jsonl\"))\n",
    "if dev_files:\n",
    "\n",
    "    devsets = {p.stem: Dataset.from_list(load_jsonl(p)) for p in dev_files}\n",
    "    print(\"\\n=== TRAINED on saved dev_* (sanity) ===\")\n",
    "    dev_scores = eval_all_testsets_with_llm(\n",
    "        trained_llm, tok, devsets,\n",
    "        system_prompt=CUSTOM_SYSTEM_PROMPT,\n",
    "        max_new_tokens=eval_max_new_tokens,\n",
    "        temperature=eval_temperature,\n",
    "    )\n",
    "    for name, acc in dev_scores.items():\n",
    "        print(f\"{name}: acc={acc:.4f} (n={len(devsets[name])})\")\n",
    "else:\n",
    "    print(\"\\n[INFO] No dev_*.jsonl files found in DATA_DIR. Skipping dev sanity eval.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e37295d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot: /home/yaros/DS-Mag/AI-SelectedTopics/W2-1/results/paired_bars_accuracy.png\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 7) Парные бары (baseline vs trained) — как в примере matplotlib\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/barchart.html\n",
    "# --------------------------\n",
    "from w2_utils import plot_paired_bars_accuracy\n",
    "\n",
    "out_png = RESULTS_DIR / \"paired_bars_accuracy.png\"\n",
    "plot_paired_bars_accuracy(\n",
    "    baseline_scores,\n",
    "    trained_scores,\n",
    "    out_path=out_png,\n",
    "    title=\"Accuracy: baseline vs trained\",\n",
    ")\n",
    "\n",
    "print(\"Saved plot:\", out_png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08eabd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot: /home/yaros/DS-Mag/AI-SelectedTopics/W2-1/results/paired_bars_accuracy.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 7) Парные бары (baseline vs trained) — как в примере matplotlib\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/barchart.html\n",
    "# --------------------------\n",
    "names = list(testsets.keys())\n",
    "base_vals = [baseline_scores[n] for n in names]\n",
    "tr_vals = [trained_scores[n] for n in names]\n",
    "\n",
    "x = list(range(len(names)))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "bars1 = ax.bar([i - width/2 for i in x], base_vals, width, label=\"Baseline (Qwen2.5-1.5B-Instruct)\")\n",
    "bars2 = ax.bar([i + width/2 for i in x], tr_vals, width, label=\"Trained (GRPO)\")\n",
    "\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Baseline vs Trained accuracy on fixed testsets\")\n",
    "ax.set_xticks(x, names)\n",
    "ax.set_ylim(0.0, 1.0)\n",
    "ax.legend(loc=\"best\")\n",
    "\n",
    "def autolabel(rects):\n",
    "    for r in rects:\n",
    "        h = r.get_height()\n",
    "        ax.annotate(\n",
    "            f\"{h:.3f}\",\n",
    "            xy=(r.get_x() + r.get_width() / 2, h),\n",
    "            xytext=(0, 3),\n",
    "            textcoords=\"offset points\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "autolabel(bars1)\n",
    "autolabel(bars2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plot_path = RESULTS_DIR / \"paired_bars_accuracy.png\"\n",
    "fig.savefig(plot_path, dpi=160)\n",
    "print(\"Saved plot:\", plot_path)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds-mag-ai-sel-top-w2-1-eval)",
   "language": "python",
   "name": "ds-mag-ai-sel-top-w2-1-eval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "name": "week2_grpo_lis_eval_vllm_v5"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
